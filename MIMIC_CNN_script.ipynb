{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from string import punctuation\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# from nltk import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertModel, BertConfig, BertTokenizer\n",
    "# from transformers import pipeline, FeatureExtractionPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext.data as data\n",
    "import torchtext.vocab as vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load BlueBERT Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Admissions Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dx codes...\n",
      "Admission: 58976\n",
      "COPD Admissions: 7459\n",
      "Num patients: 46520\n",
      "Num female: 20399\n",
      "Loading admission events...\n"
     ]
    }
   ],
   "source": [
    "# strict copd coding\n",
    "strict_icd9 = [\n",
    "    \"49120\",\n",
    "    \"49121\",\n",
    "    \"49122\",\n",
    "    \"49320\",\n",
    "    \"49321\",\n",
    "    \"49322\",\n",
    "    \"496\",\n",
    "]\n",
    "\n",
    "# regular copd coding\n",
    "reg_icd9 = [\n",
    "    \"4911\",\n",
    "    \"4920\",\n",
    "    \"4928\",\n",
    "]\n",
    "\n",
    "print(\"Loading dx codes...\")\n",
    "df = pd.read_csv(\"~/Documents/data/mimic/DIAGNOSES_ICD.csv\")\n",
    "n = df.HADM_ID.nunique()\n",
    "copd_hadmids = df[df.ICD9_CODE.isin(strict_icd9 + reg_icd9)].HADM_ID.unique()\n",
    "n_copd = len(copd_hadmids)\n",
    "print(f\"Admission: {n}\")\n",
    "print(f\"COPD Admissions: {n_copd}\")\n",
    "\n",
    "patients = pd.read_csv(\"~/Documents/data/mimic/PATIENTS.csv\", parse_dates=['DOB', 'DOD', 'DOD_HOSP'])\n",
    "print(f\"Num patients: {patients['SUBJECT_ID'].nunique()}\")\n",
    "print(f\"Num female: {patients[patients['GENDER'] == 'F']['SUBJECT_ID'].nunique()}\")\n",
    "\n",
    "admission_cols = [\n",
    "    'HADM_ID',\n",
    "    'ADMISSION_TYPE',\n",
    "    'ADMITTIME',\n",
    "    'DISCHTIME',\n",
    "    'DEATHTIME',\n",
    "    'EDREGTIME',\n",
    "    'EDOUTTIME',\n",
    "    'HOSPITAL_EXPIRE_FLAG',\n",
    "    'HAS_CHARTEVENTS_DATA',\n",
    "]\n",
    "print(\"Loading admission events...\")\n",
    "tmp = pd.read_csv(\"~/Documents/data/mimic/ADMISSIONS.csv\", parse_dates=['ADMITTIME', 'DISCHTIME','DEATHTIME', 'EDREGTIME', 'EDOUTTIME',])[admission_cols]\n",
    "\n",
    "# concat primary dx onto admissions\n",
    "admits = tmp.merge(df, on=['HADM_ID']).drop_duplicates(subset=[\"HADM_ID\"])\n",
    "\n",
    "# get rid of spurrious admissions and ignore newborns\n",
    "admits = admits[(admits['DISCHTIME'] > admits['ADMITTIME']) & (admits.ADMISSION_TYPE != \"NEWBORN\")]\n",
    "\n",
    "# add age information\n",
    "admits = admits.merge(patients[['SUBJECT_ID', 'DOB']], on='SUBJECT_ID', how='left')\n",
    "admits['age'] = admits.apply(lambda x: (x['ADMITTIME'].date() - x['DOB'].date()).days // 365.242, axis=1)\n",
    "\n",
    "# tag copd admissions\n",
    "admits['copd'] = admits.HADM_ID.isin(copd_hadmids)\n",
    "\n",
    "# get the type and time of the next admission\n",
    "admits.sort_values(by=['SUBJECT_ID', 'ADMITTIME'],inplace=True)\n",
    "admits['next_admit_time'] = admits.groupby('SUBJECT_ID').ADMITTIME.shift(-1)\n",
    "admits['next_admit_type'] = admits.groupby('SUBJECT_ID').ADMISSION_TYPE.shift(-1)\n",
    "# if the next admission is elective, nullify and back fill\n",
    "admits.loc[admits.next_admit_type == \"ELECTIVE\", 'next_admit_time'] = pd.NaT\n",
    "admits.loc[admits.next_admit_type == \"ELECTIVE\", 'next_admit_type'] = np.nan\n",
    "admits[['next_admit_time','next_admit_type']] = admits.groupby(['SUBJECT_ID'])[['next_admit_time','next_admit_type']].fillna(method = 'bfill')\n",
    "\n",
    "# compute readmission stats\n",
    "admits['readmit_time'] = admits.groupby('SUBJECT_ID').apply(lambda x: x['next_admit_time'] - x['DISCHTIME']).reset_index(level=0, drop=True)\n",
    "admits['7d_readmit'] = (admits['readmit_time'].dt.total_seconds() < 7 * 24 * 3600).astype(int)\n",
    "admits['30d_readmit'] = (admits['readmit_time'].dt.total_seconds() < 30 * 24 * 3600).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<65 Admissions\n",
      "-------------------------\n",
      "Non-COPD 7d readmit rate: 1.9%\n",
      "COPD 7d readmit rate:     3.2%\n",
      "\n",
      "Non-COPD 30d readmit rate: 5.3%\n",
      "COPD 30d readmit rate:     9.1%\n",
      "\n",
      "Non-COPD mortality rate: 7.4%\n",
      "COPD mortality rate:     8.1%\n",
      "\n",
      "\n",
      "65+ Admissions\n",
      "-------------------------\n",
      "Non-COPD 7d readmit rate: 2.2%\n",
      "COPD 7d readmit rate:     2.6%\n",
      "\n",
      "Non-COPD 30d readmit rate: 5.7%\n",
      "COPD 30d readmit rate:     7.6%\n",
      "\n",
      "Non-COPD mortality rate: 14.5%\n",
      "COPD mortality rate:     15.2%\n",
      "\n",
      "\n",
      "All Admissions\n",
      "-------------------------\n",
      "Non-COPD 7d readmit rate: 2.1%\n",
      "COPD 7d readmit rate:     2.8%\n",
      "\n",
      "Non-COPD 30d readmit rate: 5.5%\n",
      "COPD 30d readmit rate:     8.1%\n",
      "\n",
      "Non-COPD mortality rate: 10.8%\n",
      "COPD mortality rate:     13.0%\n"
     ]
    }
   ],
   "source": [
    "def print_summary(df):\n",
    "    gb = df.groupby(['copd','7d_readmit']).HADM_ID.count()\n",
    "    non_rate = gb[0][1] / gb[0].sum()\n",
    "    copd_rate = gb[1][1] / gb[1].sum()\n",
    "    print(\"Non-COPD 7d readmit rate: {:.1%}\".format(non_rate))\n",
    "    print(\"COPD 7d readmit rate:     {:.1%}\".format(copd_rate))\n",
    "    print('')\n",
    "\n",
    "    gb = df.groupby(['copd','30d_readmit']).HADM_ID.count()\n",
    "    non_rate = gb[0][1] / gb[0].sum()\n",
    "    copd_rate = gb[1][1] / gb[1].sum()\n",
    "    print(\"Non-COPD 30d readmit rate: {:.1%}\".format(non_rate))\n",
    "    print(\"COPD 30d readmit rate:     {:.1%}\".format(copd_rate))\n",
    "    print('')\n",
    "\n",
    "    gb = df[df.DEATHTIME.notnull()].drop_duplicates(subset=['SUBJECT_ID']).groupby('copd').size()\n",
    "    print(\"Non-COPD mortality rate: {:.1%}\".format(gb[0] / df[df.copd == False].shape[0]))\n",
    "    print(\"COPD mortality rate:     {:.1%}\".format(gb[1] / df[df.copd].shape[0]))\n",
    "\n",
    "print(\"<65 Admissions\")\n",
    "print(\"-\"*25)\n",
    "print_summary(admits[admits['age'] < 65])\n",
    "\n",
    "print(\"\\n\\n65+ Admissions\")\n",
    "print(\"-\"*25)\n",
    "print_summary(admits[admits['age'] >= 65])\n",
    "\n",
    "print(\"\\n\\nAll Admissions\")\n",
    "print(\"-\"*25)\n",
    "print_summary(admits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discharge Notes Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading medical notes...\n",
      "Iteration 0\n",
      "Iteration 5\n",
      "Iteration 10\n",
      "Iteration 15\n",
      "Iteration 20\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>CHARTDATE</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>DESCRIPTION</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>age</th>\n",
       "      <th>copd</th>\n",
       "      <th>HOSPITAL_EXPIRE_FLAG</th>\n",
       "      <th>ADMISSION_TYPE</th>\n",
       "      <th>ADMITTIME</th>\n",
       "      <th>DISCHTIME</th>\n",
       "      <th>DEATHTIME</th>\n",
       "      <th>next_admit_time</th>\n",
       "      <th>next_admit_type</th>\n",
       "      <th>30d_readmit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58526</td>\n",
       "      <td>100001.0</td>\n",
       "      <td>2117-09-17</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>Admission Date:  [**2117-9-11**]              ...</td>\n",
       "      <td>35.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>2117-09-11 11:46:00</td>\n",
       "      <td>2117-09-17 16:45:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2118-07-07 06:26:00</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>54610</td>\n",
       "      <td>100003.0</td>\n",
       "      <td>2150-04-21</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>Admission Date:  [**2150-4-17**]              ...</td>\n",
       "      <td>59.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>2150-04-17 15:34:00</td>\n",
       "      <td>2150-04-21 17:30:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2150-07-13 18:56:00</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9895</td>\n",
       "      <td>100006.0</td>\n",
       "      <td>2108-04-18</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Addendum</td>\n",
       "      <td>Name:  [**Known lastname 470**], [**Known firs...</td>\n",
       "      <td>48.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>2108-04-06 15:49:00</td>\n",
       "      <td>2108-04-18 17:18:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2108-08-02 15:36:00</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23018</td>\n",
       "      <td>100007.0</td>\n",
       "      <td>2145-04-07</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>Admission Date:  [**2145-3-31**]              ...</td>\n",
       "      <td>73.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>2145-03-31 05:33:00</td>\n",
       "      <td>2145-04-07 12:40:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>533</td>\n",
       "      <td>100009.0</td>\n",
       "      <td>2162-05-21</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>Admission Date:  [**2162-5-16**]              ...</td>\n",
       "      <td>60.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>2162-05-16 15:56:00</td>\n",
       "      <td>2162-05-21 13:37:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SUBJECT_ID   HADM_ID   CHARTDATE           CATEGORY DESCRIPTION  \\\n",
       "0       58526  100001.0  2117-09-17  Discharge summary      Report   \n",
       "1       54610  100003.0  2150-04-21  Discharge summary      Report   \n",
       "2        9895  100006.0  2108-04-18  Discharge summary    Addendum   \n",
       "3       23018  100007.0  2145-04-07  Discharge summary      Report   \n",
       "4         533  100009.0  2162-05-21  Discharge summary      Report   \n",
       "\n",
       "                                                TEXT   age   copd  \\\n",
       "0  Admission Date:  [**2117-9-11**]              ...  35.0  False   \n",
       "1  Admission Date:  [**2150-4-17**]              ...  59.0  False   \n",
       "2  Name:  [**Known lastname 470**], [**Known firs...  48.0   True   \n",
       "3  Admission Date:  [**2145-3-31**]              ...  73.0  False   \n",
       "4  Admission Date:  [**2162-5-16**]              ...  60.0  False   \n",
       "\n",
       "   HOSPITAL_EXPIRE_FLAG ADMISSION_TYPE           ADMITTIME  \\\n",
       "0                     0      EMERGENCY 2117-09-11 11:46:00   \n",
       "1                     0      EMERGENCY 2150-04-17 15:34:00   \n",
       "2                     0      EMERGENCY 2108-04-06 15:49:00   \n",
       "3                     0      EMERGENCY 2145-03-31 05:33:00   \n",
       "4                     0      EMERGENCY 2162-05-16 15:56:00   \n",
       "\n",
       "            DISCHTIME DEATHTIME     next_admit_time next_admit_type  \\\n",
       "0 2117-09-17 16:45:00       NaT 2118-07-07 06:26:00       EMERGENCY   \n",
       "1 2150-04-21 17:30:00       NaT 2150-07-13 18:56:00       EMERGENCY   \n",
       "2 2108-04-18 17:18:00       NaT 2108-08-02 15:36:00       EMERGENCY   \n",
       "3 2145-04-07 12:40:00       NaT                 NaT             NaN   \n",
       "4 2162-05-21 13:37:00       NaT                 NaT             NaN   \n",
       "\n",
       "   30d_readmit  \n",
       "0            0  \n",
       "1            0  \n",
       "2            0  \n",
       "3            0  \n",
       "4            0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# subjects that died in the hosp\n",
    "deceased_subj_ids = admits[admits.DEATHTIME.notnull()].SUBJECT_ID.unique()\n",
    "# subjects w/ at least one copd related admission\n",
    "copd_subj_ids = admits[admits.copd].SUBJECT_ID.unique()\n",
    "# all admissions for subjects w/ at least one copd related admission\n",
    "hadm_ids_w_copd = admits[admits.SUBJECT_ID.isin(copd_subj_ids)].HADM_ID.unique()\n",
    "\n",
    "\n",
    "print('Loading medical notes...')\n",
    "\n",
    "chunk_reader = pd.read_csv(\"~/Documents/data/mimic/NOTEEVENTS.csv\", chunksize=100000, usecols=['SUBJECT_ID','HADM_ID', 'CHARTDATE','CATEGORY', 'DESCRIPTION', 'TEXT',])\n",
    "chunk_li = []\n",
    "iteration = 0\n",
    "for chunk in chunk_reader:\n",
    "    \n",
    "    if iteration % 5 == 0:\n",
    "        print(f\"Iteration {iteration}\")\n",
    "        # keep only admissions for subjects that had at least one copd admit\n",
    "#     chunk_li.append(chunk[(chunk['HADM_ID'].isin(hadm_ids_w_copd))])\n",
    "        # keep just the discharge summaries\n",
    "    chunk_li.append(chunk[(chunk['CATEGORY'] == 'Discharge summary')])\n",
    "    iteration += 1\n",
    "    \n",
    "print(\"Done.\")\n",
    "\n",
    "notes = pd.concat(chunk_li, ignore_index=True)\n",
    "# keep only one discharge summary per admission\n",
    "notes = notes.sort_values(by=['SUBJECT_ID', 'HADM_ID', 'CHARTDATE']).groupby(['HADM_ID']).nth(-1)\n",
    "cols = ['HADM_ID', 'SUBJECT_ID','age', 'copd', 'HOSPITAL_EXPIRE_FLAG','ADMISSION_TYPE', 'ADMITTIME', 'DISCHTIME', 'DEATHTIME','next_admit_time', 'next_admit_type','30d_readmit',]\n",
    "notes = notes.merge(admits[cols], on=['SUBJECT_ID','HADM_ID'], how='inner')\n",
    "notes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Process Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Admission Date:  <DATE>              Discharge Date:   <DATE>  Date of Birth:  <DATE>             Sex:   F  Service: MEDICINE  Allergies: Levaquin  Attending:<NAME> Chief Complaint: nausea, vomiting   Major Surgical or Invasive Procedure: none  History of Present Illness: 35F w/ poorly controlled Type 1 diabetes mellitus w/ neuropathy, nephropathy, HTN, gastroparesis, CKD and retinopathy, recently hospitalized for orthostatic hypotension <NUMBER> autonomic neuropathy <UNK>; DKA hospitalizations in <NUMBER> and <NUMBER>, now returning w/ 5d history of worsening nausea, vomiting with coffee-ground emesis, chills, and dyspnea on exertion.  Last week she had a fall and hit her right face.  she also had 1 day of diarrhea, which resolved early last week.  Found to be in DKA with AG 30 and bicarb 11. . In the ED inital vitals were 09:00 0 98.2 113 181/99 22 100% RA. K 4.7, HCO3 11, Anion Gap 30, Cr. 2.7 (baseline 1.6-2.0) She is on her 3rd L NS. Insulin srip at 5 units/hr. On home at 22 levemir in am and 12 at with difficult to control sugars. BPs have been high. Given 30 mtroprolol tartrate in ED.  She was started on an insulin drip at 5 units/hr and 3L NS boluses. Also aspirin 325mg PO and Morphine 4mg IVx1 for pain. CXr was clear.  EKG NAD. . Review of systems: otherwise negative.  Past Medical History: Type 1 diabetes mellitis w/ neuropathy, nephropathy, and retinopathy - 2 episodes of DKA in <NUMBER> and <NUMBER> HTN - 5 years gastroparesis - 1.5 years CKD - stage III, baseline Cr 2.4-2.5, proteinuria L1 vertebral fracture - <DATE> Systolic ejection murmur  Social History: Patient lives at home in <UNK> with her 8 y/o daughter and boyfriend. She has no history of EtOH, tobacco, or illicit drug use. She is currently unemployed and seeking disability.   Family History: Both parents have HTN and T2DM. Grandfather had an MI in his 40s.  Physical Exam: GEN: Awake, alert, and oriented HEENT: PERRLA. MMM. no JVD. neck supple. No cervical LAD Cards: RRR, S1/S2 normal. II/VI systolic ejection murmur heard best at the L upper sternal border. Pulm: CTABL with no crackles or wheezes. Abd: BS+, soft, NT, no rebound/guarding, no HSM, no <NAME> sign Extremities: wwp, no edema. radials, DPs, PTs 2+. Skin: no rashes or bruising. no skin tenting. Neuro: CNs II-XII intact. Upper extremities: Power <NUMBER> bilaterally. Le: left power: 4.5/5  right: power <NUMBER>.  Bilateral symmetric, reduced sensation distal LE to ankles.   Pertinent Results: Admission Labs: <DATE> 09:22AM W'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_text(df, char_limit=2500):\n",
    "    # This function preprocesses the text by filling not a number \n",
    "    # and replacing new lines ('\\n') and carriage returns ('\\r')\n",
    "    text_data = df.TEXT\\\n",
    "        .fillna(' ')\\\n",
    "        .str.replace('\\n', ' ')\\\n",
    "        .str.replace('\\r', ' ')\\\n",
    "        .str.replace('\\[\\*\\*(\\d{4}-\\d{1,2}-\\d{1,2})\\*\\*\\]',\n",
    "                     '<DATE>', regex=True)\\\n",
    "        .str.replace(r'\\[\\*\\*([\\da-zA-Z() \\(\\)]*?(?:Name|name)[\\da-zA-Z \\(\\)]*?)\\*\\*\\]',\n",
    "                     '<NAME>', regex=True)\\\n",
    "        .str.replace(r'\\[\\*\\*([\\d-]*?)\\*\\*\\]',\n",
    "                     '<NUMBER>', regex=True)\\\n",
    "        .str.replace(r'\\[\\*\\*(Hospital.*?)\\*\\*\\]',\n",
    "                     '<HOSPITAL>', regex=True)\\\n",
    "        .str.replace(r'\\[\\*\\*(.*?)\\*\\*\\]',\n",
    "                     '<UNK>', regex=True)\\\n",
    "        .str[:char_limit]\\\n",
    "    \n",
    "    return text_data\n",
    "\n",
    "# discard admissions where the patient died during the stay\n",
    "# use 30d readmission as the label\n",
    "text_data = preprocess_text(notes[notes.HOSPITAL_EXPIRE_FLAG == 0])\n",
    "y = notes[notes.HOSPITAL_EXPIRE_FLAG == 0]['30d_readmit']\n",
    "# text_data.str.len().describe()\n",
    "text_data.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('~/Documents/data/BlueBERT/NCBI_BERT_pubmed_mimic_uncased_L-12_H-768_A-12')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "Path(\"~/Documents/data/BlueBERT/NCBI_BERT_pubmed_mimic_uncased_L-12_H-768_A-12/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Admission Date:  &lt;DATE&gt;              Discharge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Admission Date:  &lt;DATE&gt;              Discharge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Name:  &lt;NAME&gt;, &lt;NAME&gt;                        U...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Admission Date:  &lt;DATE&gt;              Discharge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Admission Date:  &lt;DATE&gt;              Discharge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>Admission Date:  &lt;DATE&gt;     Discharge Date:  &lt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>Admission Date:  &lt;DATE&gt;              Discharge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>Admission Date:  &lt;DATE&gt;              Discharge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>Admission Date:  &lt;DATE&gt;              Discharge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>Admission Date:  &lt;DATE&gt;              Discharge...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   TEXT\n",
       "0     Admission Date:  <DATE>              Discharge...\n",
       "1     Admission Date:  <DATE>              Discharge...\n",
       "2     Name:  <NAME>, <NAME>                        U...\n",
       "3     Admission Date:  <DATE>              Discharge...\n",
       "4     Admission Date:  <DATE>              Discharge...\n",
       "...                                                 ...\n",
       "4995  Admission Date:  <DATE>     Discharge Date:  <...\n",
       "4996  Admission Date:  <DATE>              Discharge...\n",
       "4997  Admission Date:  <DATE>              Discharge...\n",
       "4998  Admission Date:  <DATE>              Discharge...\n",
       "4999  Admission Date:  <DATE>              Discharge...\n",
       "\n",
       "[5000 rows x 1 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_data = pd.read_csv(\"~/Documents/data/mimic/mimic_discharge_summaries_2500chars_subset.csv\")\n",
    "tmp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Admission Date:  [**2117-9-11**]              Discharge Date:   [**2117-9-17**]\\n\\nDate of Birth:  [**2082-3-21**]             Sex:   F\\n\\nService: MEDICINE\\n\\nAllergies:\\nLevaquin\\n\\nAttending:[**First Name3 (LF) 2195**]\\nChief Complaint:\\nnausea, vomiting\\n\\n\\nMajor Surgical or Invasive Procedure:\\nnone\\n\\nHistory of Present Illness:\\n35F w/ poorly controlled Type 1 diabetes mellitus w/ neuropathy,\\nnephropathy, HTN, gastroparesis, CKD and retinopathy, recently\\nhospitalized for orthostatic hypotension [**2-3**] autonomic\\nneuropathy [**Date range (1) 25088**]; DKA hospitalizations in [**6-12**] and [**7-12**], now\\nreturning w/ 5d history of worsening nausea, vomiting with\\ncoffee-ground emesis, chills, and dyspnea on exertion.  Last\\nweek she had a fall and hit her right face.  she also had 1 day\\nof diarrhea, which resolved early last week.  Found to be in DKA\\nwith AG 30 and bicarb 11.\\n.\\nIn the ED inital vitals were 09:00 0 98.2 113 181/99 22 100% RA.\\nK 4.7, HCO3 11, Anion Gap 30, Cr. 2.7 (baseline 1.6-2.0) She is\\non her 3rd L NS. Insulin srip at 5 units/hr. On home at 22\\nlevemir in am and 12 at with difficult to control sugars. BPs\\nhave been high. Given 30 mtroprolol tartrate in ED.\\n\\nShe was started on an insulin drip at 5 units/hr and 3L NS\\nboluses. Also aspirin 325mg PO and Morphine 4mg IVx1 for pain.\\nCXr was clear.  EKG NAD.\\n.\\nReview of systems: otherwise negative.\\n\\nPast Medical History:\\nType 1 diabetes mellitis w/ neuropathy, nephropathy, and\\nretinopathy - 2 episodes of DKA in [**6-12**] and [**7-12**]\\nHTN - 5 years\\ngastroparesis - 1.5 years\\nCKD - stage III, baseline Cr 2.4-2.5, proteinuria\\nL1 vertebral fracture - [**2117-7-17**]\\nSystolic ejection murmur\\n\\nSocial History:\\nPatient lives at home in [**Location (un) **] with her 8 y/o daughter and\\nboyfriend. She has no history of EtOH, tobacco, or illicit drug\\nuse. She is currently unemployed and seeking disability.\\n\\n\\nFamily History:\\nBoth parents have HTN and T2DM. Grandfather had an MI in his\\n40s.\\n\\nPhysical Exam:\\nGEN: Awake, alert, and oriented\\nHEENT: PERRLA. MMM. no JVD. neck supple. No cervical LAD\\nCards: RRR, S1/S2 normal. II/VI systolic ejection murmur heard\\nbest at the L upper sternal border.\\nPulm: CTABL with no crackles or wheezes.\\nAbd: BS+, soft, NT, no rebound/guarding, no HSM, no [**Doctor Last Name 515**]\\nsign\\nExtremities: wwp, no edema. radials, DPs, PTs 2+.\\nSkin: no rashes or bruising. no skin tenting.\\nNeuro: CNs II-XII intact. Upper extremities: Power [**5-6**]\\nbilaterally. Le: left power: 4.5/5  right: power [**3-6**].  Bilateral\\nsymmetric, reduced sensation distal LE to ankles.\\n\\n\\nPertinent Results:\\nAdmission Labs: [**2117-9-11**] 09:22AM\\nWBC-11.9* RBC-4.58 HGB-13.0 HCT-36.5 MCV-80* PLT COUNT-466*\\nLIPASE-22  ALT(SGPT)-10 AST(SGOT)-16 ALK PHOS-105 TOT BILI-0.5\\nGLUCOSE-260* UREA N-48* CREAT-2.7* SODIUM-137 POTASSIUM-4.9\\nCL-101 CO2-11*\\nLACTATE-1.9\\n\\nDischarge Labs: [**2117-9-16**] 07:10AM\\nWBC-6.8 RBC-3.67* Hgb-10.4* Hct-30.2* MCV-82 Plt Ct-298\\nGlucose-118* UreaN-20 Creat-2.3* Na-137 K-3.7 Cl-104 HCO3-23\\nAnGap-14\\nCalcium-8.7 Phos-3.5 Mg-2.0\\n\\nRadiology:\\nCXR: No evidence of pneumonia or other pathological\\nabnormalities. No\\npleural effusions. No pulmonary edema. Normal size of the\\ncardiac\\nsilhouette.\\n\\nMicrobiology: Urine culture negative, blood cultures no growth\\nto date, stool for C.difficile negative\\n\\n\\nBrief Hospital Course:\\n35 yo F with HTN & poorly controlled type I DM, c/b neuropathy,\\ngastroparesis, nephropathy ?????? CKD, retinopathy presents with DKA\\nand hypertension SBP to 200s.\\n.\\n# Diabetic ketoacidosis: Patient controls diabetes at home with\\nHumalog SS and long acting Levemir.  Sugars at home recently\\nhave been in 250s. In the ED, glucose was 466. UA was +ve for\\nketones ?????? corrected to 200s, but rose again to 300s. She was\\ntreated with an insulin drip which was transitioned to subq when\\nshe tolerated POs. Her electrolytes were repleted and she\\nreceived aggressive volume resuscitation. [**Last Name (un) **] saw her and\\ngave sliding scale recommendations which were implemented. No\\nsource for DKA found, beleived to be [**2-3**] gastroparesis. Nausea\\nmanaged with ativan, compazine, and promethazine. She was\\ndischarged on her home Insulin and sliding scale with\\ninstructions to follow-up with [**Last Name (un) **].\\n\\n# HTN: Hypertensive with SBP in 190s initially, attributed to\\nDKA, as she has experienced in the past. As she improved her\\nblood pressures normalized and she was re-started on her home\\nLopressor and Midodrine regimen.\\n\\n# Coffee grounds emesis: Emesis started off as clear, then with\\nprolonged wretching, she started having coffee-grounds vomiting.\\nThis had also occurred on prior admissions for DKA with\\nassociated vomiting. Her hematocrit remained stable and her\\nhematemesis self-resolved, and so work-up was deferred to the\\noutpatient setting.\\n\\n# Acute on chronic kidney disease, Stage III: Patient's Cr on\\nadmission was 2.7, trending down to 2.1-2.3 following fluids,\\nconsistent with her known CKD secondary to diabetic nephropathy.\\n\\n\\nMedications on Admission:\\n1. citalopram 20 mg Tablet Sig: One (1) Tablet PO DAILY (Daily).\\n\\n2. Levemir 100 unit/mL Solution Sig: Twenty Two (22) units\\nSubcutaneous every AM.\\n3. Levemir 100 unit/mL Solution Sig: Twelve (12) units\\nSubcutaneous at bedtime.\\n4. Humalog 100 unit/mL Solution Sig: sliding scale as directed\\nSubcutaneous four times a day: Please use sliding scale as\\ndirected by MD [**First Name8 (NamePattern2) 767**] [**Last Name (Titles) **].\\n5. metoprolol tartrate 50 mg Tablet Sig: 1.5 Tablets PO DAILY\\n(Daily): take in the evening.\\n6. promethazine 25 mg Tablet Sig: 0.5 Tablet PO Q8H (every 8\\nhours) as needed for nausea.\\n7. gabapentin 300 mg Capsule Sig: One (1) Capsule PO Q12H (every\\n\\n12 hours).\\nDisp:*60 Capsule(s)* Refills:*2*\\n8. duloxetine 30 mg Capsule, Delayed Release(E.C.) Sig: Two (2)\\nCapsule, Delayed Release(E.C.) PO DAILY (Daily): Please take\\nonly 1 capsule daily (30 mg) for first 2 weeks of treatment.\\nDisp:*60 Capsule, Delayed Release(E.C.)(s)* Refills:*2*\\n9. oxycodone 5 mg Capsule Sig: One (1) Capsule PO every eight\\n(8) hours as needed for pain.\\n10. midodrine 5 mg Tablet Sig: 1.5 Tablets PO every four (4)\\nhours: Can hold while sleeping.\\nDisp:*270 Tablet(s)* Refills:*2*\\n\\n\\nDischarge Medications:\\n1. citalopram 20 mg Tablet Sig: One (1) Tablet PO DAILY (Daily).\\n\\n2. gabapentin 300 mg Capsule Sig: One (1) Capsule PO Q12H (every\\n12 hours).\\n3. duloxetine 30 mg Capsule, Delayed Release(E.C.) Sig: One (1)\\nCapsule, Delayed Release(E.C.) PO DAILY (Daily).\\n4. metoprolol tartrate 25 mg Tablet Sig: Three (3) Tablet PO\\nOnce Daily at 6 PM.\\n5. midodrine 2.5 mg Tablet Sig: Three (3) Tablet PO DAILY\\n(Daily).\\n6. Levemir 100 unit/mL Solution Sig: As directed by [**Last Name (un) **] units\\nSubcutaneous As directed.\\n\\n\\nDischarge Disposition:\\nHome\\n\\nDischarge Diagnosis:\\nDiabetic keotacidosis\\nHematemesis (blood in your vomit)\\nHypertension\\nChronic renal insufficiency\\n\\nDischarge Condition:\\nMental Status: Clear and coherent.\\nLevel of Consciousness: Alert and interactive.\\nActivity Status: Ambulatory - Independent.\\n\\nDischarge Instructions:\\nYou were admitted to the hospital with DKA, hypertension, and\\nblood in your vomit. You were initially treated in the ICU with\\nan insulin drip, and your blood sugars improved. Your blood\\npressure medications were adjusted to better control your blood\\npressure while you were in DKA, but you were re-started on your\\nhome regimen at discharge. The blood in your vomit was likely\\nsecondary to mechanical trauma from repeated wretching, but you\\nshould follow-up with your primary care doctor to discuss\\nwhether you should undergo further evaluation such as an upper\\nendoscopy. Given your complaints of chronic cough and heartburn,\\nyou should also discuss beginning a trial of a proton pump\\ninhibitor such as Nexium or Prilosec to see if this helps your\\nsymptoms.\\n\\nYour insulin regimen was adjusted by the [**Last Name (un) **] team while you\\nwere here. You should continue to follow-up with them with any\\nquestions or concerns regarding your insulin management.\\n\\nFollowup Instructions:\\nPlease call Dr.[**Last Name (STitle) 805**]' office to schedule a follow-up\\nappointment within 7-10 days of discharge. Her office number is\\n[**Telephone/Fax (1) 85219**].\\n\\nYou should also continue to follow-up with your [**Last Name (un) **] doctors\\nas needed.\\n\\n\\n\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_string[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Admission Date:  <DATE>              Discharge Date:   <DATE>\\n\\nDate of Birth:  <DATE>             Sex:   F\\n\\nService: MEDICINE\\n\\nAllergies:\\nLevaquin\\n\\nAttending:<NAME>\\nChief Complaint:\\nnausea, vomiting\\n\\n\\nMajor Surgical or Invasive Procedure:\\nnone\\n\\nHistory of Present Illness:\\n35F w/ poorly controlled Type 1 diabetes mellitus w/ neuropathy,\\nnephropathy, HTN, gastroparesis, CKD and retinopathy, recently\\nhospitalized for orthostatic hypotension <NUMBER> autonomic\\nneuropathy <UNK>; DKA hospitalizations in <NUMBER> and <NUMBER>, now\\nreturning w/ 5d history of worsening nausea, vomiting with\\ncoffee-ground emesis, chills, and dyspnea on exertion.  Last\\nweek she had a fall and hit her right face.  she also had 1 day\\nof diarrhea, which resolved early last week.  Found to be in DKA\\nwith AG 30 and bicarb 11.\\n.\\nIn the ED inital vitals were 09:00 0 98.2 113 181/99 22 100% RA.\\nK 4.7, HCO3 11, Anion Gap 30, Cr. 2.7 (baseline 1.6-2.0) She is\\non her 3rd L NS. Insulin srip at 5 units/hr. On home at 22\\nlevemir in am and 12 at with difficult to control sugars. BPs\\nhave been high. Given 30 mtroprolol tartrate in ED.\\n\\nShe was started on an insulin drip at 5 units/hr and 3L NS\\nboluses. Also aspirin 325mg PO and Morphine 4mg IVx1 for pain.\\nCXr was clear.  EKG NAD.\\n.\\nReview of systems: otherwise negative.\\n\\nPast Medical History:\\nType 1 diabetes mellitis w/ neuropathy, nephropathy, and\\nretinopathy - 2 episodes of DKA in <NUMBER> and <NUMBER>\\nHTN - 5 years\\ngastroparesis - 1.5 years\\nCKD - stage III, baseline Cr 2.4-2.5, proteinuria\\nL1 vertebral fracture - <DATE>\\nSystolic ejection murmur\\n\\nSocial History:\\nPatient lives at home in <UNK> with her 8 y/o daughter and\\nboyfriend. She has no history of EtOH, tobacco, or illicit drug\\nuse. She is currently unemployed and seeking disability.\\n\\n\\nFamily History:\\nBoth parents have HTN and T2DM. Grandfather had an MI in his\\n40s.\\n\\nPhysical Exam:\\nGEN: Awake, alert, and oriented\\nHEENT: PERRLA. MMM. no JVD. neck supple. No cervical LAD\\nCards: RRR, S1/S2 normal. II/VI systolic ejection murmur heard\\nbest at the L upper sternal border.\\nPulm: CTABL with no crackles or wheezes.\\nAbd: BS+, soft, NT, no rebound/guarding, no HSM, no <NAME>\\nsign\\nExtremities: wwp, no edema. radials, DPs, PTs 2+.\\nSkin: no rashes or bruising. no skin tenting.\\nNeuro: CNs II-XII intact. Upper extremities: Power <NUMBER>\\nbilaterally. Le: left power: 4.5/5  right: power <NUMBER>.  Bilateral\\nsymmetric, reduced sensation distal LE to ankles.\\n\\n\\nPertinent Results:\\nAdmission Labs: <DATE> 09:22AM\\nWBC-11.9* RBC-4.58 HGB-13.0 HCT-36.5 MCV-80* PLT COUNT-466*\\nLIPASE-22  ALT(SGPT)-10 AST(SGOT)-16 ALK PHOS-105 TOT BILI-0.5\\nGLUCOSE-260* UREA N-48* CREAT-2.7* SODIUM-137 POTASSIUM-4.9\\nCL-101 CO2-11*\\nLACTATE-1.9\\n\\nDischarge Labs: <DATE> 07:10AM\\nWBC-6.8 RBC-3.67* Hgb-10.4* Hct-30.2* MCV-82 Plt Ct-298\\nGlucose-118* UreaN-20 Creat-2.3* Na-137 K-3.7 Cl-104 HCO3-23\\nAnGap-14\\nCalcium-8.7 Phos-3.5 Mg-2.0\\n\\nRadiology:\\nCXR: No evidence of pneumonia or other pathological\\nabnormalities. No\\npleural effusions. No pulmonary edema. Normal size of the\\ncardiac\\nsilhouette.\\n\\nMicrobiology: Urine culture negative, blood cultures no growth\\nto date, stool for C.difficile negative\\n\\n\\nBrief Hospital Course:\\n35 yo F with HTN & poorly controlled type I DM, c/b neuropathy,\\ngastroparesis, nephropathy ?????? CKD, retinopathy presents with DKA\\nand hypertension SBP to 200s.\\n.\\n# Diabetic ketoacidosis: Patient controls diabetes at home with\\nHumalog SS and long acting Levemir.  Sugars at home recently\\nhave been in 250s. In the ED, glucose was 466. UA was +ve for\\nketones ?????? corrected to 200s, but rose again to 300s. She was\\ntreated with an insulin drip which was transitioned to subq when\\nshe tolerated POs. Her electrolytes were repleted and she\\nreceived aggressive volume resuscitation. <NAME> saw her and\\ngave sliding scale recommendations which were implemented. No\\nsource for DKA found, beleived to be <NUMBER> gastroparesis. Nausea\\nmanaged with ativan, compazine, and promethazine. She was\\ndischarged on her home Insulin and sliding scale with\\ninstructions to follow-up with <NAME>.\\n\\n# HTN: Hypertensive with SBP in 190s initially, attributed to\\nDKA, as she has experienced in the past. As she improved her\\nblood pressures normalized and she was re-started on her home\\nLopressor and Midodrine regimen.\\n\\n# Coffee grounds emesis: Emesis started off as clear, then with\\nprolonged wretching, she started having coffee-grounds vomiting.\\nThis had also occurred on prior admissions for DKA with\\nassociated vomiting. Her hematocrit remained stable and her\\nhematemesis self-resolved, and so work-up was deferred to the\\noutpatient setting.\\n\\n# Acute on chronic kidney disease, Stage III: Patient's Cr on\\nadmission was 2.7, trending down to 2.1-2.3 following fluids,\\nconsistent with her known CKD secondary to diabetic nephropathy.\\n\\n\\nMedications on Admission:\\n1. citalopram 20 mg Tablet Sig: One (1) Tablet PO DAILY (Daily).\\n\\n2. Levemir 100 unit/mL Solution Sig: Twenty Two (22) units\\nSubcutaneous every AM.\\n3. Levemir 100 unit/mL Solution Sig: Twelve (12) units\\nSubcutaneous at bedtime.\\n4. Humalog 100 unit/mL Solution Sig: sliding scale as directed\\nSubcutaneous four times a day: Please use sliding scale as\\ndirected by MD <NAME> <NAME>.\\n5. metoprolol tartrate 50 mg Tablet Sig: 1.5 Tablets PO DAILY\\n(Daily): take in the evening.\\n6. promethazine 25 mg Tablet Sig: 0.5 Tablet PO Q8H (every 8\\nhours) as needed for nausea.\\n7. gabapentin 300 mg Capsule Sig: One (1) Capsule PO Q12H (every\\n\\n12 hours).\\nDisp:*60 Capsule(s)* Refills:*2*\\n8. duloxetine 30 mg Capsule, Delayed Release(E.C.) Sig: Two (2)\\nCapsule, Delayed Release(E.C.) PO DAILY (Daily): Please take\\nonly 1 capsule daily (30 mg) for first 2 weeks of treatment.\\nDisp:*60 Capsule, Delayed Release(E.C.)(s)* Refills:*2*\\n9. oxycodone 5 mg Capsule Sig: One (1) Capsule PO every eight\\n(8) hours as needed for pain.\\n10. midodrine 5 mg Tablet Sig: 1.5 Tablets PO every four (4)\\nhours: Can hold while sleeping.\\nDisp:*270 Tablet(s)* Refills:*2*\\n\\n\\nDischarge Medications:\\n1. citalopram 20 mg Tablet Sig: One (1) Tablet PO DAILY (Daily).\\n\\n2. gabapentin 300 mg Capsule Sig: One (1) Capsule PO Q12H (every\\n12 hours).\\n3. duloxetine 30 mg Capsule, Delayed Release(E.C.) Sig: One (1)\\nCapsule, Delayed Release(E.C.) PO DAILY (Daily).\\n4. metoprolol tartrate 25 mg Tablet Sig: Three (3) Tablet PO\\nOnce Daily at 6 PM.\\n5. midodrine 2.5 mg Tablet Sig: Three (3) Tablet PO DAILY\\n(Daily).\\n6. Levemir 100 unit/mL Solution Sig: As directed by <NAME> units\\nSubcutaneous As directed.\\n\\n\\nDischarge Disposition:\\nHome\\n\\nDischarge Diagnosis:\\nDiabetic keotacidosis\\nHematemesis (blood in your vomit)\\nHypertension\\nChronic renal insufficiency\\n\\nDischarge Condition:\\nMental Status: Clear and coherent.\\nLevel of Consciousness: Alert and interactive.\\nActivity Status: Ambulatory - Independent.\\n\\nDischarge Instructions:\\nYou were admitted to the hospital with DKA, hypertension, and\\nblood in your vomit. You were initially treated in the ICU with\\nan insulin drip, and your blood sugars improved. Your blood\\npressure medications were adjusted to better control your blood\\npressure while you were in DKA, but you were re-started on your\\nhome regimen at discharge. The blood in your vomit was likely\\nsecondary to mechanical trauma from repeated wretching, but you\\nshould follow-up with your primary care doctor to discuss\\nwhether you should undergo further evaluation such as an upper\\nendoscopy. Given your complaints of chronic cough and heartburn,\\nyou should also discuss beginning a trial of a proton pump\\ninhibitor such as Nexium or Prilosec to see if this helps your\\nsymptoms.\\n\\nYour insulin regimen was adjusted by the <NAME> team while you\\nwere here. You should continue to follow-up with them with any\\nquestions or concerns regarding your insulin management.\\n\\nFollowup Instructions:\\nPlease call Dr.<NAME>' office to schedule a follow-up\\nappointment within 7-10 days of discharge. Her office number is\\n<UNK>.\\n\\nYou should also continue to follow-up with your <NAME> doctors\\nas needed.\\n\\n\\n\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "## date tag <DATE>\n",
    "# test_string.str.extractall(r'\\[\\*\\*(\\d{4}-\\d{1,2}-\\d{1,2})\\*\\*\\]')\n",
    "\n",
    "## Name tag <NAME>\n",
    "# test_string.str.extractall(r'\\[\\*\\*([\\da-zA-Z() \\(\\)]*?(?:Name|name)[\\da-zA-Z \\(\\)]*?)\\*\\*\\]')\n",
    "\n",
    "## number tag  <NUMBER>\n",
    "# test_string.str.extractall(r'\\[\\*\\*([\\d-]*?)\\*\\*\\]')\n",
    "\n",
    "## hospital tag <HOSPITAL>\n",
    "# test_string.str.extractall(r'\\[\\*\\*(Hospital.*?)\\*\\*\\]')\n",
    "\n",
    "## remainder will be <UNK>\n",
    "# test_string.str.extractall(r'\\[\\*\\*(.*?)\\*\\*\\]').head(30)\n",
    "\n",
    "\n",
    "# date tag <DATE>\n",
    "test_string = test_string.str.replace('\\[\\*\\*(\\d{4}-\\d{1,2}-\\d{1,2})\\*\\*\\]', '<DATE>', regex=True)\n",
    "\n",
    "# Name tag <NAME>\n",
    "test_string = test_string.str.replace(r'\\[\\*\\*([\\da-zA-Z() \\(\\)]*?(?:Name|name)[\\da-zA-Z \\(\\)]*?)\\*\\*\\]', '<NAME>', regex=True)\n",
    "\n",
    "# number tag  <NUMBER>\n",
    "test_string = test_string.str.replace(r'\\[\\*\\*([\\d-]*?)\\*\\*\\]', '<NUMBER>', regex=True)\n",
    "\n",
    "# hospital tag <HOSPITAL>\n",
    "test_string = test_string.str.replace(r'\\[\\*\\*(Hospital.*?)\\*\\*\\]', '<HOSPITAL>', regex=True)\n",
    "\n",
    "# remainder will be <UNK>\n",
    "test_string = test_string.str.replace(r'\\[\\*\\*(.*?)\\*\\*\\]', '<UNK>', regex=True)\n",
    "\n",
    "test_string[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toy example of using BERT and transformers library to tokenize and generate embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]           101\n",
      "Prior         4,602\n",
      "to            1,106\n",
      "the           1,103\n",
      "hospital      2,704\n",
      "##ization     2,734\n",
      ",               117\n",
      "she           1,131\n",
      "had           1,125\n",
      "a               170\n",
      "L               149\n",
      "par          14,247\n",
      "##ot          3,329\n",
      "##ide         3,269\n",
      "##ct          5,822\n",
      "##omy        18,574\n",
      "for           1,111\n",
      "what          1,184\n",
      "turned        1,454\n",
      "out           1,149\n",
      "to            1,106\n",
      "be            1,129\n",
      "par          14,247\n",
      "##ot          3,329\n",
      "##id          2,386\n",
      "##itis       10,721\n",
      "and           1,105\n",
      "si           27,466\n",
      "##ala         5,971\n",
      "##den         2,883\n",
      "##itis       10,721\n",
      "with          1,114\n",
      "a               170\n",
      "large         1,415\n",
      "retained      5,366\n",
      "duct         26,862\n",
      "stone         2,576\n",
      ".               119\n",
      "Ultimately   16,266\n",
      ",               117\n",
      "it            1,122\n",
      "became        1,245\n",
      "clear         2,330\n",
      "she           1,131\n",
      "had           1,125\n",
      "no            1,185\n",
      "persistent   15,970\n",
      "infectious   20,342\n",
      "process       1,965\n",
      "in            1,107\n",
      "the           1,103\n",
      "par          14,247\n",
      "##ot          3,329\n",
      "##id          2,386\n",
      "bed           1,908\n",
      ",               117\n",
      "but           1,133\n",
      "had           1,125\n",
      "evolving     23,657\n",
      "car           1,610\n",
      "##ba          2,822\n",
      "##pen        11,741\n",
      "##em          5,521\n",
      "and           1,105\n",
      "c               172\n",
      "##ep          8,043\n",
      "##hal         7,654\n",
      "##os          2,155\n",
      "##por        18,876\n",
      "##in          1,394\n",
      "er           14,044\n",
      "##yt         25,669\n",
      "##hr          8,167\n",
      "##ode        13,040\n",
      "##rm          9,019\n",
      ".               119\n",
      "Her           1,430\n",
      "r               187\n",
      "##ash        10,733\n",
      "##es          1,279\n",
      "improved      4,725\n",
      "dramatically 12,235\n",
      "with          1,114\n",
      "transition    6,468\n",
      "to            1,106\n",
      "from          1,121\n",
      "me            1,143\n",
      "##rop        12,736\n",
      "##ene         7,582\n",
      "##m           1,306\n",
      "to            1,106\n",
      "c               172\n",
      "##ef         11,470\n",
      "##ep          8,043\n",
      "##ime        10,453\n",
      "to            1,106\n",
      "a               170\n",
      "##z           1,584\n",
      "##tre         7,877\n",
      "##ona         7,637\n",
      "##m           1,306\n",
      ".               119\n",
      "Her           1,430\n",
      "course        1,736\n",
      "was           1,108\n",
      "further       1,748\n",
      "complicated   8,277\n",
      "by            1,118\n",
      "a               170\n",
      "fever        10,880\n",
      "curve         7,660\n",
      "that          1,115\n",
      "had           1,125\n",
      "regular       2,366\n",
      "T               157\n",
      "##max        22,871\n",
      "in            1,107\n",
      "the           1,103\n",
      "101           7,393\n",
      "range         2,079\n",
      ",               117\n",
      "re            1,231\n",
      "##sol        24,313\n",
      "##ving        3,970\n",
      "while         1,229\n",
      "on            1,113\n",
      "van           3,498\n",
      "##com         8,178\n",
      "##y           1,183\n",
      "##cin        16,430\n",
      ",               117\n",
      "a               170\n",
      "##z           1,584\n",
      "##tre         7,877\n",
      "##ona         7,637\n",
      "##m           1,306\n",
      ",               117\n",
      "c               172\n",
      "##lind       27,969\n",
      "##am          2,312\n",
      "##y           1,183\n",
      "##cin        16,430\n",
      "and           1,105\n",
      "mi            1,940\n",
      "##ca          2,599\n",
      "##fu         14,703\n",
      "##ng          2,118\n",
      "##in          1,394\n",
      ",               117\n",
      "but           1,133\n",
      "[101, 4602, 1106, 1103, 2704, 2734, 117, 1131, 1125, 170, 149, 14247, 3329, 3269, 5822, 18574, 1111, 1184, 1454, 1149, 1106, 1129, 14247, 3329, 2386, 10721, 1105, 27466, 5971, 2883, 10721, 1114, 170, 1415, 5366, 26862, 2576, 119, 16266, 117, 1122, 1245, 2330, 1131, 1125, 1185, 15970, 20342, 1965, 1107, 1103, 14247, 3329, 2386, 1908, 117, 1133, 1125, 23657, 1610, 2822, 11741, 5521, 1105, 172, 8043, 7654, 2155, 18876, 1394, 14044, 25669, 8167, 13040, 9019, 119, 1430, 187, 10733, 1279, 4725, 12235, 1114, 6468, 1106, 1121, 1143, 12736, 7582, 1306, 1106, 172, 11470, 8043, 10453, 1106, 170, 1584, 7877, 7637, 1306, 119, 1430, 1736, 1108, 1748, 8277, 1118, 170, 10880, 7660, 1115, 1125, 2366, 157, 22871, 1107, 1103, 7393, 2079, 117, 1231, 24313, 3970, 1229, 1113, 3498, 8178, 1183, 16430, 117, 170, 1584, 7877, 7637, 1306, 117, 172, 27969, 2312, 1183, 16430, 1105, 1940, 2599, 14703, 2118, 1394, 117, 1133]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8689f804229e431daa6d9ecad3d65a73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3829875dd3274c668360b43719caa80b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-110-2e58918cac62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0msegments_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msegments_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bert-base-uncased'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/nlp_env/lib/python3.7/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    830\u001b[0m                     \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m                     \u001b[0mresume_download\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_download\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m                     \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m                 )\n\u001b[1;32m    834\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mresolved_archive_file\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/nlp_env/lib/python3.7/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, local_files_only)\u001b[0m\n\u001b[1;32m    688\u001b[0m             \u001b[0mresume_download\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_download\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m             \u001b[0muser_agent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muser_agent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 690\u001b[0;31m             \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    691\u001b[0m         )\n\u001b[1;32m    692\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_or_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/nlp_env/lib/python3.7/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mget_from_cache\u001b[0;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, local_files_only)\u001b[0m\n\u001b[1;32m    867\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s not found in cache or force_download set to True, downloading to %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 869\u001b[0;31m             \u001b[0mhttp_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresume_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_agent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muser_agent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    870\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storing %s in cache at %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/nlp_env/lib/python3.7/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, user_agent)\u001b[0m\n\u001b[1;32m    760\u001b[0m         \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_verbosity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNOTSET\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m     )\n\u001b[0;32m--> 762\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# filter out keep-alive new chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m             \u001b[0mprogress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/nlp_env/lib/python3.7/site-packages/requests/models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'stream'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    752\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mProtocolError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/nlp_env/lib/python3.7/site-packages/urllib3/response.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    574\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_fp_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 576\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecode_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/nlp_env/lib/python3.7/site-packages/urllib3/response.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m                 \u001b[0mcache_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfp_closed\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m                 if (\n\u001b[1;32m    521\u001b[0m                     \u001b[0mamt\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/nlp_env/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    455\u001b[0m             \u001b[0;31m# Amount is given, implement using readinto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/nlp_env/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    499\u001b[0m         \u001b[0;31m# connection, and the user is reading more bytes than will be provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m         \u001b[0;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 501\u001b[0;31m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m             \u001b[0;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/nlp_env/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/nlp_env/lib/python3.7/site-packages/urllib3/contrib/pyopenssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOpenSSL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSSL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSysCallError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Unexpected EOF\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/nlp_env/lib/python3.7/site-packages/OpenSSL/SSL.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1837\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSSL_peek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ssl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1838\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1839\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSSL_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ssl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1840\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_ssl_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ssl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "text = \"\"\"Prior to the hospitalization, she had a L parotidectomy for what turned out to be parotiditis \n",
    "    and sialadenitis with a large  retained duct stone. Ultimately, it became clear she had no persistent\n",
    "    infectious process in the parotid bed, but had evolving carbapenem and cephalosporin erythroderm.\n",
    "    Her rashes improved dramatically with transition to from meropenem to cefepime to aztreonam. Her course\n",
    "    was further complicated by a fever curve that had regular Tmax in the 101 range, resolving while on \n",
    "    vancomycin, aztreonam, clindamycin and micafungin, but  then recurred first low grade then becoming \n",
    "    very hectic and high  grade to 104 without any focal findings. The vancomycin was stopped and she \n",
    "    defervesced after 72 hours. She soon thereafter recovered her counts and all antibiotics were \n",
    "    discontinued when her ANC approached 500.\"\"\"\n",
    "bert_base_tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "# 1. Add the special tokens\n",
    "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "\n",
    "# 2. Split the sentence into tokens\n",
    "tokenized_text = bert_base_tokenizer.tokenize(marked_text)\n",
    "tokenized_text = tokenized_text[:150]\n",
    "\n",
    "# 3. Map the token strings to their vocabulary indices\n",
    "indexed_tokens = bert_base_tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "# 4. Display the words with their indices.\n",
    "for tup in zip(tokenized_text, indexed_tokens):\n",
    "    print('{:<12} {:>6,}'.format(tup[0], tup[1]))\n",
    "\n",
    "# 5. Segment IDs: 0 for sentence 1 and 1 for sentence 2\n",
    "segments_ids = [1] * len(tokenized_text)\n",
    "\n",
    "print(indexed_tokens)\n",
    "print(segments_ids)\n",
    "# 6. convert the lists to tensors\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    last_hidden_state, pooler_output = model(tokens_tensor, segments_tensors)\n",
    "last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 150, 768])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-trained BlueBERT model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "blue_bert_tokenizer = BertTokenizer.from_pretrained(\"/Users/kevin/Documents/data/BlueBERT/NCBI_BERT_pubmed_mimic_uncased_L-12_H-768_A-12/\")\n",
    "configuration = BertConfig.from_json_file(\"/Users/kevin/Documents/data/BlueBERT/NCBI_BERT_pubmed_mimic_uncased_L-12_H-768_A-12/bert_config.json\")\n",
    "model = BertModel.from_pretrained(\"/Users/kevin/Documents/data/BlueBERT/NCBI_BERT_pubmed_mimic_uncased_L-12_H-768_A-12/pytorch_model.bin\", config=configuration)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer requires input text to be either a single string or list of strings\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "tokens = blue_bert_tokenizer(\n",
    "    text_data[:10].values.tolist(),\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    max_length=128,\n",
    "    return_tensors=\"pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 128])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing batch 1...\n",
      "Tokenizing batch 101...\n",
      "Tokenizing batch 201...\n",
      "Tokenizing batch 301...\n",
      "Tokenizing batch 401...\n",
      "Tokenizing batch 501...\n",
      "Tokenizing batch 601...\n",
      "Tokenizing batch 701...\n",
      "Tokenizing batch 801...\n",
      "Tokenizing batch 901...\n",
      "Tokenizing batch 1001...\n",
      "Tokenizing batch 1101...\n",
      "Tokenizing batch 1201...\n",
      "Tokenizing batch 1301...\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "max_seq_length = 64\n",
    "token_li = []\n",
    "for i in range(text_data.shape[0] // batch_size + 1):\n",
    "    start_idx = i * 32\n",
    "    stop_idx = (i + 1) * 32\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(f\"Tokenizing batch {i + 1}...\")\n",
    "    # shape: (32, 128)\n",
    "    tokens = blue_bert_tokenizer(\n",
    "        text_data[start_idx:stop_idx].values.tolist(),\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_seq_length,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    token_li.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding batch 1...\n",
      "Embedding batch 26...\n",
      "Embedding batch 51...\n",
      "Embedding batch 76...\n",
      "Embedding batch 101...\n",
      "Embedding batch 126...\n",
      "Embedding batch 151...\n",
      "Embedding batch 176...\n",
      "Embedding batch 201...\n",
      "Embedding batch 226...\n",
      "Embedding batch 251...\n",
      "Embedding batch 276...\n",
      "Embedding batch 301...\n",
      "Embedding batch 326...\n",
      "Embedding batch 351...\n",
      "Embedding batch 376...\n",
      "Embedding batch 401...\n",
      "Embedding batch 426...\n",
      "Embedding batch 451...\n",
      "Embedding batch 476...\n",
      "Embedding batch 501...\n",
      "Embedding batch 526...\n",
      "Embedding batch 551...\n",
      "Embedding batch 576...\n",
      "Embedding batch 601...\n",
      "Embedding batch 626...\n",
      "Embedding batch 651...\n",
      "Embedding batch 676...\n",
      "Embedding batch 701...\n",
      "Embedding batch 726...\n",
      "Embedding batch 751...\n",
      "Embedding batch 776...\n",
      "Embedding batch 801...\n",
      "Embedding batch 826...\n",
      "Embedding batch 851...\n",
      "Embedding batch 876...\n",
      "Embedding batch 901...\n",
      "Embedding batch 926...\n",
      "Embedding batch 951...\n",
      "Embedding batch 976...\n",
      "Embedding batch 1001...\n",
      "Embedding batch 1026...\n",
      "Embedding batch 1051...\n",
      "Embedding batch 1076...\n",
      "Embedding batch 1101...\n",
      "Embedding batch 1126...\n",
      "Embedding batch 1151...\n",
      "Embedding batch 1176...\n",
      "Embedding batch 1201...\n",
      "Embedding batch 1226...\n",
      "Embedding batch 1251...\n",
      "Embedding batch 1276...\n",
      "Embedding batch 1301...\n",
      "Embedding batch 1326...\n",
      "Embedding batch 1351...\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "hidden_states = []\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(token_li):\n",
    "        if i % 25 == 0:\n",
    "            print(f\"Embedding batch {i + 1}...\")\n",
    "        last_hidden_state, _ = model(**batch)\n",
    "        hidden_states.append(last_hidden_state)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([43875, 64, 768])\n"
     ]
    }
   ],
   "source": [
    "embeddings = torch.cat(hidden_states)\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "write(): fd 64 failed with No space left on device",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-97-588b19d1c05d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/Users/kevin/Documents/data/mimic/bluebert_dc_notes_64tkns.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/miniconda3/envs/nlp_env/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m         \u001b[0m_legacy_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/nlp_env/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_legacy_save\u001b[0;34m(obj, f, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mserialized_storage_keys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m         \u001b[0mserialized_storages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_write_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_should_read_directly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: write(): fd 64 failed with No space left on device"
     ]
    }
   ],
   "source": [
    "torch.save(embeddings, \"/Users/kevin/Documents/data/mimic/bluebert_dc_notes_64tkns.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor size: 8.03 GB\n"
     ]
    }
   ],
   "source": [
    "def print_size(tensor, unit='MB'):\n",
    "    n_bytes = tensor.element_size() * tensor.nelement()\n",
    "    n_mb = n_bytes / (1024.0 * 1024.0)\n",
    "    n_gb = n_bytes / (1024.0 * 1024.0 * 1024.0)\n",
    "    if unit == \"MB\":\n",
    "        print(f\"Tensor size: {n_mb:0.2f} MB\")\n",
    "    elif unit == \"GB\":\n",
    "        print(f\"Tensor size: {n_gb:0.2f} GB\")\n",
    "\n",
    "# (embeddings.element_size() * embeddings.nelement()) // (1024.0 * 1024.0 * 1024.0)\n",
    "\n",
    "print_size(embeddings, unit=\"GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_size(embeddings, unit='GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_list = [0,1,2,3,4,5,6,7,8]\n",
    "my_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevin/opt/miniconda3/envs/nlp_env/lib/python3.7/site-packages/ipykernel_launcher.py:11: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-6812a3db739a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmods\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmods\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mout_sizes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0minput_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/nlp_env/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "model = TextCNN(\n",
    "        num_classes=2,\n",
    "        embedding_size=768,\n",
    "        num_filters=128,\n",
    "        dropout_rate=0.5,\n",
    "    )\n",
    "\n",
    "input_ = Variable(torch.ones(128,1,128,768), volatile=True)\n",
    "mods = list(model.modules())\n",
    "out_sizes = []\n",
    "for i in range(1, len(mods)):\n",
    "    m = mods[i]\n",
    "    out = m(input_)\n",
    "    out_sizes.append(np.array(out.size()))\n",
    "    input_ = out\n",
    "\n",
    "total_bits = 0\n",
    "for i in range(len(self.out_sizes)):\n",
    "    s = self.out_sizes[i]\n",
    "    bits = np.prod(np.array(s))*self.bits\n",
    "    total_bits += bits\n",
    "\n",
    "# multiply by 2\n",
    "# we need to store values AND gradients\n",
    "total_bits *= 2\n",
    "print(total_bits) # 4595712"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          ...,\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.]]],\n",
       "\n",
       "\n",
       "        [[[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          ...,\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.]]],\n",
       "\n",
       "\n",
       "        [[[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          ...,\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          ...,\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.]]],\n",
       "\n",
       "\n",
       "        [[[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          ...,\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.]]],\n",
       "\n",
       "\n",
       "        [[[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          ...,\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.]]]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28584"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(vocab.keys()) & set(wv.vocab.keys()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x83 in position 10: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-5f18262563f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# wv = gensim.models.KeyedVectors.load_word2vec_format('/Users/kevin/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz', binary=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# BlueBERT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mwv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Users/kevin/gensim-data/NCBI_BERT_pubmed_mimic_uncased_L-12_H-768_A-12.zip'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/nlp_env/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m   1496\u001b[0m         return _load_word2vec_format(\n\u001b[1;32m   1497\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1498\u001b[0;31m             limit=limit, datatype=datatype)\n\u001b[0m\u001b[1;32m   1499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_keras_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/nlp_env/lib/python3.7/site-packages/gensim/models/utils_any2vec.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m    341\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loading projection weights from %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m         \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m         \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# throws for invalid file format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/nlp_env/lib/python3.7/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36many2unicode\u001b[0;34m(text, encoding, errors)\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0municode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x83 in position 10: invalid start byte"
     ]
    }
   ],
   "source": [
    "wv = gensim.models.KeyedVectors.load_word2vec_format('/Users/kevin/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz', binary=True)\n",
    "# BlueBERT\n",
    "# wv = gensim.models.KeyedVectors.load_word2vec_format('/Users/kevin/gensim-data/NCBI_BERT_pubmed_mimic_uncased_L-12_H-768_A-12.zip', binary=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vitals\n",
      "lf\n",
      "bs\n",
      "cxr\n",
      "found\n",
      "parents\n",
      "guarding\n",
      "aspirin\n",
      "clear\n",
      "nad\n"
     ]
    }
   ],
   "source": [
    "for i, word in enumerate(vocab):\n",
    "    if i == 10:\n",
    "        break\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'car'\t'vehicle'\t0.78\n",
      "'car'\t'minivan'\t0.69\n",
      "'car'\t'bicycle'\t0.54\n",
      "'car'\t'airplane'\t0.42\n",
      "'car'\t'cereal'\t0.14\n",
      "'car'\t'communism'\t0.06\n"
     ]
    }
   ],
   "source": [
    "pairs = [\n",
    "    ('car', 'vehicle'),\n",
    "    ('car', 'minivan'),   # a minivan is a kind of car\n",
    "    ('car', 'bicycle'),   # still a wheeled vehicle\n",
    "    ('car', 'airplane'),  # ok, no wheels, but still a vehicle\n",
    "    ('car', 'cereal'),    # ... and so on\n",
    "    ('car', 'communism'),\n",
    "]\n",
    "for w1, w2 in pairs:\n",
    "    print('%r\\t%r\\t%.2f' % (w1, w2, wv.similarity(w1, w2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word2vec(tokens_list, vector, generate_missing=False, k=300):\n",
    "    if generate_missing:\n",
    "        vectorized = [vector[word] if word in vector else np.random.uniform(-0.25,0.25,k) for word in tokens_list]\n",
    "    else:\n",
    "        vectorized = [vector[word] if word in vector else np.zeros(k) for word in tokens_list]\n",
    "    return vectorized\n",
    "sample_sequence = get_word2vec(tokens[0], wv, generate_missing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(sample_sequence)\n",
    "sample_sequence[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(nn.Module):\n",
    "    \"\"\" Text CNN from Yoon Kim's 2014 paper: https://arxiv.org/pdf/1408.5882.pdf\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    sequence_length : int\n",
    "        The length (in words/tokens) of each sentence.\n",
    "    num_classes : int\n",
    "        Number of output classes.\n",
    "    vocab_size : int\n",
    "        The number of unique tokens in our vocabulary.\n",
    "    embedding_size: int\n",
    "        The vector length for word embeddings. (standard word2vec is 300, BERT is 768)\n",
    "    num_filters: int\n",
    "        The number of filters to apply.\n",
    "    kernel_sizes: tuple(int)\n",
    "        The kernel size for each desired filter (e.g. [3,4,5])\n",
    "    dropout_rate: float\n",
    "        Probability of dropping a neuron in the dropout layer.  Must be in the range [0.0, 1.0]\n",
    "        Default = 0.5\n",
    "    embedding_weights: torch.FloatTensor\n",
    "        Pre-trained embedding weights to optionally pass, otherwise embedding weights will be learned.\n",
    "        Default is None.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    model : nn.Module\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes,\n",
    "        embedding_size,\n",
    "        num_filters,\n",
    "        kernel_sizes=(3,4,5),\n",
    "        dropout_rate=0.5,\n",
    "    ):\n",
    "        super(TextCNN, self).__init__()\n",
    "        \n",
    "        # convolutional layer\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(\n",
    "            in_channels=1,\n",
    "            out_channels=num_filters,\n",
    "            kernel_size=(kernel_size, embedding_size),\n",
    "        ) for kernel_size in kernel_sizes])\n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # fully connected layer\n",
    "        self.fc = nn.Linear(\n",
    "            in_features=num_filters * len(kernel_sizes),\n",
    "            out_features=num_classes\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # (batch_size, in_channels, sequence_length, embedding_size)\n",
    "        \n",
    "        x_li = []\n",
    "        for conv in self.convs:\n",
    "            _x = F.relu(conv(x)) # (batch_size, out_channels, sequence_length, 1)\n",
    "            _x = _x.squeeze(3) # (batch_size, out_channels, sequence_length)\n",
    "            _x = F.max_pool1d(_x, _x.size(2)).squeeze(2) # (batch_size, out_channels)\n",
    "            x_li.append(_x)\n",
    "            \n",
    "        x = torch.cat(x_li, 1)\n",
    "        x = self.dropout(x) # (batch_size, len(kernel_sizes) * out_channels)\n",
    "        logits = self.fc(x) # (batch_size, num_classes)\n",
    "        \n",
    "        probs = F.softmax(logits, dim=1) # (batch_size, num_classes)\n",
    "        classes = torch.max(probs, 1)[1] # (batch_size)\n",
    "        \n",
    "        return probs, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert training data to pytorch dataset\n",
    "train_tensors = torch.utils.data.TensorDataset(\n",
    "    embeddings,\n",
    "    torch.tensor(y.values).long()\n",
    ")\n",
    "\n",
    "# Create iterable\n",
    "trainloader = torch.utils.data.DataLoader(train_tensors, batch_size=32,\n",
    "                                          shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model = TextCNN(\n",
    "        num_classes=2,\n",
    "        embedding_size=768,\n",
    "        num_filters=128,\n",
    "        dropout_rate=0.5,\n",
    "    )\n",
    "    \n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "            \n",
    "    for epoch in range(5):  # loop over the dataset multiple times\n",
    "\n",
    "        running_loss = 0\n",
    "        running_corrects = 0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward\n",
    "            probs, classes = model(inputs)\n",
    "            # backprop\n",
    "            loss = loss_function(probs, labels)\n",
    "            loss.backward()\n",
    "            # update/optimize\n",
    "            optimizer.step()\n",
    "\n",
    "#             running_losses.append(loss.data[0])\n",
    "#             if i % 50 == 0:\n",
    "#                 disp_loss = sum(running_losses) / len(running_losses)\n",
    "# #                 writer.add_scalar(\"train/loss\", loss, step)\n",
    "#                 logging.info(\"step = {}, loss = {}\".format(i, loss))\n",
    "#                 running_losses = []\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            running_corrects += torch.sum(classes == labels.data)\n",
    "            if i % 50 == 0 and i != 0:    # print every 50 mini-batches\n",
    "                print('[%d, %5d] loss: %.3f acc %.3f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 50, running_corrects / 50))\n",
    "                running_loss = 0.0\n",
    "                running_corrects = 0.0\n",
    "                \n",
    "#             epoch_loss = running_loss / dataset_sizes[phase]\n",
    "#             epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevin/opt/miniconda3/envs/nlp_env/lib/python3.7/site-packages/ipykernel_launcher.py:74: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    51] loss: 0.389 acc 30.000\n",
      "[1,   101] loss: 0.383 acc 29.780\n",
      "[1,   151] loss: 0.376 acc 29.980\n",
      "[1,   201] loss: 0.385 acc 29.720\n",
      "[1,   251] loss: 0.381 acc 29.840\n",
      "[1,   301] loss: 0.377 acc 29.960\n",
      "[1,   351] loss: 0.371 acc 30.160\n",
      "[1,   401] loss: 0.385 acc 29.700\n",
      "[2,    51] loss: 0.384 acc 30.000\n",
      "[2,   101] loss: 0.380 acc 29.880\n",
      "[2,   151] loss: 0.381 acc 29.820\n",
      "[2,   201] loss: 0.380 acc 29.880\n",
      "[2,   251] loss: 0.372 acc 30.120\n",
      "[2,   301] loss: 0.380 acc 29.860\n",
      "[2,   351] loss: 0.376 acc 30.000\n",
      "[2,   401] loss: 0.382 acc 29.800\n",
      "[3,    51] loss: 0.381 acc 30.000\n",
      "[3,   101] loss: 0.379 acc 29.900\n",
      "[3,   151] loss: 0.372 acc 30.120\n",
      "[3,   201] loss: 0.392 acc 29.480\n",
      "[3,   251] loss: 0.385 acc 29.700\n",
      "[3,   301] loss: 0.376 acc 30.000\n",
      "[3,   351] loss: 0.381 acc 29.820\n",
      "[3,   401] loss: 0.366 acc 30.320\n",
      "[4,    51] loss: 0.378 acc 30.000\n",
      "[4,   101] loss: 0.376 acc 29.980\n",
      "[4,   151] loss: 0.376 acc 29.980\n",
      "[4,   201] loss: 0.387 acc 29.640\n",
      "[4,   251] loss: 0.378 acc 29.920\n",
      "[4,   301] loss: 0.390 acc 29.540\n",
      "[4,   351] loss: 0.373 acc 30.100\n",
      "[4,   401] loss: 0.375 acc 30.040\n",
      "[5,    51] loss: 0.391 acc 30.000\n",
      "[5,   101] loss: 0.380 acc 29.880\n",
      "[5,   151] loss: 0.380 acc 29.880\n",
      "[5,   201] loss: 0.369 acc 30.220\n",
      "[5,   251] loss: 0.391 acc 29.500\n",
      "[5,   301] loss: 0.369 acc 30.220\n",
      "[5,   351] loss: 0.376 acc 29.980\n",
      "[5,   401] loss: 0.377 acc 29.960\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
