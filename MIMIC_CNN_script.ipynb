{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from string import punctuation\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# from nltk import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertModel, BertConfig, BertTokenizer\n",
    "# from transformers import pipeline, FeatureExtractionPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext.data as data\n",
    "import torchtext.vocab as vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load BlueBERT Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Admissions Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dx codes...\n",
      "Admission: 58976\n",
      "COPD Admissions: 7459\n",
      "Num patients: 46520\n",
      "Num female: 20399\n",
      "Loading admission events...\n"
     ]
    }
   ],
   "source": [
    "# strict copd coding\n",
    "strict_icd9 = [\n",
    "    \"49120\",\n",
    "    \"49121\",\n",
    "    \"49122\",\n",
    "    \"49320\",\n",
    "    \"49321\",\n",
    "    \"49322\",\n",
    "    \"496\",\n",
    "]\n",
    "\n",
    "# regular copd coding\n",
    "reg_icd9 = [\n",
    "    \"4911\",\n",
    "    \"4920\",\n",
    "    \"4928\",\n",
    "]\n",
    "\n",
    "print(\"Loading dx codes...\")\n",
    "df = pd.read_csv(\"~/Documents/data/mimic/DIAGNOSES_ICD.csv\")\n",
    "n = df.HADM_ID.nunique()\n",
    "copd_hadmids = df[df.ICD9_CODE.isin(strict_icd9 + reg_icd9)].HADM_ID.unique()\n",
    "n_copd = len(copd_hadmids)\n",
    "print(f\"Admission: {n}\")\n",
    "print(f\"COPD Admissions: {n_copd}\")\n",
    "\n",
    "patients = pd.read_csv(\"~/Documents/data/mimic/PATIENTS.csv\", parse_dates=['DOB', 'DOD', 'DOD_HOSP'])\n",
    "print(f\"Num patients: {patients['SUBJECT_ID'].nunique()}\")\n",
    "print(f\"Num female: {patients[patients['GENDER'] == 'F']['SUBJECT_ID'].nunique()}\")\n",
    "\n",
    "admission_cols = [\n",
    "    'HADM_ID',\n",
    "    'ADMISSION_TYPE',\n",
    "    'ADMITTIME',\n",
    "    'DISCHTIME',\n",
    "    'DEATHTIME',\n",
    "    'EDREGTIME',\n",
    "    'EDOUTTIME',\n",
    "    'HOSPITAL_EXPIRE_FLAG',\n",
    "    'HAS_CHARTEVENTS_DATA',\n",
    "]\n",
    "print(\"Loading admission events...\")\n",
    "tmp = pd.read_csv(\"~/Documents/data/mimic/ADMISSIONS.csv\", parse_dates=['ADMITTIME', 'DISCHTIME','DEATHTIME', 'EDREGTIME', 'EDOUTTIME',])[admission_cols]\n",
    "\n",
    "# concat primary dx onto admissions\n",
    "admits = tmp.merge(df, on=['HADM_ID']).drop_duplicates(subset=[\"HADM_ID\"])\n",
    "\n",
    "# get rid of spurrious admissions and ignore newborns\n",
    "admits = admits[(admits['DISCHTIME'] > admits['ADMITTIME']) & (admits.ADMISSION_TYPE != \"NEWBORN\")]\n",
    "\n",
    "# add age information\n",
    "admits = admits.merge(patients[['SUBJECT_ID', 'DOB']], on='SUBJECT_ID', how='left')\n",
    "admits['age'] = admits.apply(lambda x: (x['ADMITTIME'].date() - x['DOB'].date()).days // 365.242, axis=1)\n",
    "\n",
    "# tag copd admissions\n",
    "admits['copd'] = admits.HADM_ID.isin(copd_hadmids)\n",
    "\n",
    "# get the type and time of the next admission\n",
    "admits.sort_values(by=['SUBJECT_ID', 'ADMITTIME'],inplace=True)\n",
    "admits['next_admit_time'] = admits.groupby('SUBJECT_ID').ADMITTIME.shift(-1)\n",
    "admits['next_admit_type'] = admits.groupby('SUBJECT_ID').ADMISSION_TYPE.shift(-1)\n",
    "# if the next admission is elective, nullify and back fill\n",
    "admits.loc[admits.next_admit_type == \"ELECTIVE\", 'next_admit_time'] = pd.NaT\n",
    "admits.loc[admits.next_admit_type == \"ELECTIVE\", 'next_admit_type'] = np.nan\n",
    "admits[['next_admit_time','next_admit_type']] = admits.groupby(['SUBJECT_ID'])[['next_admit_time','next_admit_type']].fillna(method = 'bfill')\n",
    "\n",
    "# compute readmission stats\n",
    "admits['readmit_time'] = admits.groupby('SUBJECT_ID').apply(lambda x: x['next_admit_time'] - x['DISCHTIME']).reset_index(level=0, drop=True)\n",
    "admits['7d_readmit'] = (admits['readmit_time'].dt.total_seconds() < 7 * 24 * 3600).astype(int)\n",
    "admits['30d_readmit'] = (admits['readmit_time'].dt.total_seconds() < 30 * 24 * 3600).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<65 Admissions\n",
      "-------------------------\n",
      "Non-COPD 7d readmit rate: 1.9%\n",
      "COPD 7d readmit rate:     3.2%\n",
      "\n",
      "Non-COPD 30d readmit rate: 5.3%\n",
      "COPD 30d readmit rate:     9.1%\n",
      "\n",
      "Non-COPD mortality rate: 7.4%\n",
      "COPD mortality rate:     8.1%\n",
      "\n",
      "\n",
      "65+ Admissions\n",
      "-------------------------\n",
      "Non-COPD 7d readmit rate: 2.2%\n",
      "COPD 7d readmit rate:     2.6%\n",
      "\n",
      "Non-COPD 30d readmit rate: 5.7%\n",
      "COPD 30d readmit rate:     7.6%\n",
      "\n",
      "Non-COPD mortality rate: 14.5%\n",
      "COPD mortality rate:     15.2%\n",
      "\n",
      "\n",
      "All Admissions\n",
      "-------------------------\n",
      "Non-COPD 7d readmit rate: 2.1%\n",
      "COPD 7d readmit rate:     2.8%\n",
      "\n",
      "Non-COPD 30d readmit rate: 5.5%\n",
      "COPD 30d readmit rate:     8.1%\n",
      "\n",
      "Non-COPD mortality rate: 10.8%\n",
      "COPD mortality rate:     13.0%\n"
     ]
    }
   ],
   "source": [
    "def print_summary(df):\n",
    "    gb = df.groupby(['copd','7d_readmit']).HADM_ID.count()\n",
    "    non_rate = gb[0][1] / gb[0].sum()\n",
    "    copd_rate = gb[1][1] / gb[1].sum()\n",
    "    print(\"Non-COPD 7d readmit rate: {:.1%}\".format(non_rate))\n",
    "    print(\"COPD 7d readmit rate:     {:.1%}\".format(copd_rate))\n",
    "    print('')\n",
    "\n",
    "    gb = df.groupby(['copd','30d_readmit']).HADM_ID.count()\n",
    "    non_rate = gb[0][1] / gb[0].sum()\n",
    "    copd_rate = gb[1][1] / gb[1].sum()\n",
    "    print(\"Non-COPD 30d readmit rate: {:.1%}\".format(non_rate))\n",
    "    print(\"COPD 30d readmit rate:     {:.1%}\".format(copd_rate))\n",
    "    print('')\n",
    "\n",
    "    gb = df[df.DEATHTIME.notnull()].drop_duplicates(subset=['SUBJECT_ID']).groupby('copd').size()\n",
    "    print(\"Non-COPD mortality rate: {:.1%}\".format(gb[0] / df[df.copd == False].shape[0]))\n",
    "    print(\"COPD mortality rate:     {:.1%}\".format(gb[1] / df[df.copd].shape[0]))\n",
    "\n",
    "print(\"<65 Admissions\")\n",
    "print(\"-\"*25)\n",
    "print_summary(admits[admits['age'] < 65])\n",
    "\n",
    "print(\"\\n\\n65+ Admissions\")\n",
    "print(\"-\"*25)\n",
    "print_summary(admits[admits['age'] >= 65])\n",
    "\n",
    "print(\"\\n\\nAll Admissions\")\n",
    "print(\"-\"*25)\n",
    "print_summary(admits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discharge Notes Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading medical notes...\n",
      "Iteration 0\n",
      "Iteration 5\n",
      "Iteration 10\n",
      "Iteration 15\n",
      "Iteration 20\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>CHARTDATE</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>DESCRIPTION</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>age</th>\n",
       "      <th>copd</th>\n",
       "      <th>HOSPITAL_EXPIRE_FLAG</th>\n",
       "      <th>ADMISSION_TYPE</th>\n",
       "      <th>ADMITTIME</th>\n",
       "      <th>DISCHTIME</th>\n",
       "      <th>DEATHTIME</th>\n",
       "      <th>next_admit_time</th>\n",
       "      <th>next_admit_type</th>\n",
       "      <th>30d_readmit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58526</td>\n",
       "      <td>100001.0</td>\n",
       "      <td>2117-09-17</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>Admission Date:  [**2117-9-11**]              ...</td>\n",
       "      <td>35.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>2117-09-11 11:46:00</td>\n",
       "      <td>2117-09-17 16:45:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2118-07-07 06:26:00</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>54610</td>\n",
       "      <td>100003.0</td>\n",
       "      <td>2150-04-21</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>Admission Date:  [**2150-4-17**]              ...</td>\n",
       "      <td>59.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>2150-04-17 15:34:00</td>\n",
       "      <td>2150-04-21 17:30:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2150-07-13 18:56:00</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9895</td>\n",
       "      <td>100006.0</td>\n",
       "      <td>2108-04-18</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Addendum</td>\n",
       "      <td>Name:  [**Known lastname 470**], [**Known firs...</td>\n",
       "      <td>48.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>2108-04-06 15:49:00</td>\n",
       "      <td>2108-04-18 17:18:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2108-08-02 15:36:00</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23018</td>\n",
       "      <td>100007.0</td>\n",
       "      <td>2145-04-07</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>Admission Date:  [**2145-3-31**]              ...</td>\n",
       "      <td>73.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>2145-03-31 05:33:00</td>\n",
       "      <td>2145-04-07 12:40:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>533</td>\n",
       "      <td>100009.0</td>\n",
       "      <td>2162-05-21</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>Admission Date:  [**2162-5-16**]              ...</td>\n",
       "      <td>60.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>2162-05-16 15:56:00</td>\n",
       "      <td>2162-05-21 13:37:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SUBJECT_ID   HADM_ID   CHARTDATE           CATEGORY DESCRIPTION  \\\n",
       "0       58526  100001.0  2117-09-17  Discharge summary      Report   \n",
       "1       54610  100003.0  2150-04-21  Discharge summary      Report   \n",
       "2        9895  100006.0  2108-04-18  Discharge summary    Addendum   \n",
       "3       23018  100007.0  2145-04-07  Discharge summary      Report   \n",
       "4         533  100009.0  2162-05-21  Discharge summary      Report   \n",
       "\n",
       "                                                TEXT   age   copd  \\\n",
       "0  Admission Date:  [**2117-9-11**]              ...  35.0  False   \n",
       "1  Admission Date:  [**2150-4-17**]              ...  59.0  False   \n",
       "2  Name:  [**Known lastname 470**], [**Known firs...  48.0   True   \n",
       "3  Admission Date:  [**2145-3-31**]              ...  73.0  False   \n",
       "4  Admission Date:  [**2162-5-16**]              ...  60.0  False   \n",
       "\n",
       "   HOSPITAL_EXPIRE_FLAG ADMISSION_TYPE           ADMITTIME  \\\n",
       "0                     0      EMERGENCY 2117-09-11 11:46:00   \n",
       "1                     0      EMERGENCY 2150-04-17 15:34:00   \n",
       "2                     0      EMERGENCY 2108-04-06 15:49:00   \n",
       "3                     0      EMERGENCY 2145-03-31 05:33:00   \n",
       "4                     0      EMERGENCY 2162-05-16 15:56:00   \n",
       "\n",
       "            DISCHTIME DEATHTIME     next_admit_time next_admit_type  \\\n",
       "0 2117-09-17 16:45:00       NaT 2118-07-07 06:26:00       EMERGENCY   \n",
       "1 2150-04-21 17:30:00       NaT 2150-07-13 18:56:00       EMERGENCY   \n",
       "2 2108-04-18 17:18:00       NaT 2108-08-02 15:36:00       EMERGENCY   \n",
       "3 2145-04-07 12:40:00       NaT                 NaT             NaN   \n",
       "4 2162-05-21 13:37:00       NaT                 NaT             NaN   \n",
       "\n",
       "   30d_readmit  \n",
       "0            0  \n",
       "1            0  \n",
       "2            0  \n",
       "3            0  \n",
       "4            0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# subjects that died in the hosp\n",
    "deceased_subj_ids = admits[admits.DEATHTIME.notnull()].SUBJECT_ID.unique()\n",
    "# subjects w/ at least one copd related admission\n",
    "copd_subj_ids = admits[admits.copd].SUBJECT_ID.unique()\n",
    "# all admissions for subjects w/ at least one copd related admission\n",
    "hadm_ids_w_copd = admits[admits.SUBJECT_ID.isin(copd_subj_ids)].HADM_ID.unique()\n",
    "\n",
    "\n",
    "print('Loading medical notes...')\n",
    "\n",
    "chunk_reader = pd.read_csv(\"~/Documents/data/mimic/NOTEEVENTS.csv\", chunksize=100000, usecols=['SUBJECT_ID','HADM_ID', 'CHARTDATE','CATEGORY', 'DESCRIPTION', 'TEXT',])\n",
    "chunk_li = []\n",
    "iteration = 0\n",
    "for chunk in chunk_reader:\n",
    "    \n",
    "    if iteration % 5 == 0:\n",
    "        print(f\"Iteration {iteration}\")\n",
    "        # keep only admissions for subjects that had at least one copd admit\n",
    "#     chunk_li.append(chunk[(chunk['HADM_ID'].isin(hadm_ids_w_copd))])\n",
    "        # keep just the discharge summaries\n",
    "        chunk_li.append(chunk[(chunk['CATEGORY'] == 'Discharge summary')])\n",
    "    iteration += 1\n",
    "    \n",
    "print(\"Done.\")\n",
    "\n",
    "notes = pd.concat(chunk_li, ignore_index=True)\n",
    "# keep only one discharge summary per admission\n",
    "notes = notes.sort_values(by=['SUBJECT_ID', 'HADM_ID', 'CHARTDATE']).groupby(['HADM_ID']).nth(-1)\n",
    "cols = ['HADM_ID', 'SUBJECT_ID','age', 'copd', 'HOSPITAL_EXPIRE_FLAG','ADMISSION_TYPE', 'ADMITTIME', 'DISCHTIME', 'DEATHTIME','next_admit_time', 'next_admit_type','30d_readmit',]\n",
    "notes = notes.merge(admits[cols], on=['SUBJECT_ID','HADM_ID'], how='inner')\n",
    "notes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Process Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    13162.000000\n",
       "mean      2401.058958\n",
       "std        374.180210\n",
       "min        409.000000\n",
       "25%       2500.000000\n",
       "50%       2500.000000\n",
       "75%       2500.000000\n",
       "max       2500.000000\n",
       "Name: TEXT, dtype: float64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_text(df):\n",
    "    # This function preprocesses the text by filling not a number \n",
    "    # and replacing new lines ('\\n') and carriage returns ('\\r')\n",
    "    df.TEXT = df.TEXT.fillna(' ')\n",
    "    df.TEXT = df.TEXT.str.replace('\\n',' ')\n",
    "    df.TEXT = df.TEXT.str.replace('\\r',' ')\n",
    "    return df\n",
    "\n",
    "# discard admissions where the patient died during the stay\n",
    "# use 30d readmission as the label\n",
    "text_data = preprocess_text(notes[notes.HOSPITAL_EXPIRE_FLAG == 0].sample(frac=0.3, random_state=42))['TEXT'].str[:2500]\n",
    "y = notes[notes.HOSPITAL_EXPIRE_FLAG == 0].sample(frac=0.3, random_state=42)['30d_readmit']\n",
    "text_data.str.len().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31181    Admission Date:  [**2149-11-4**]              ...\n",
       "28182    Admission Date:  [**2137-3-21**]              ...\n",
       "21728    Admission Date:  [**2109-7-6**]              D...\n",
       "45550    Name:  [**Known lastname **], [**Known firstna...\n",
       "34146    Admission Date:  [**2176-2-8**]       Discharg...\n",
       "                               ...                        \n",
       "7409     Admission Date:  [**2106-11-24**]             ...\n",
       "151      Admission Date: [**2126-2-7**]        Discharg...\n",
       "32593    Admission Date: [**2199-6-7**]        Discharg...\n",
       "48852    Admission Date:  [**2200-12-12**]             ...\n",
       "10715    Admission Date:  [**2146-1-20**]     Discharge...\n",
       "Name: TEXT, Length: 13162, dtype: object"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toy example of using BERT and transformers library to tokenize and generate embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]           101\n",
      "Prior         4,602\n",
      "to            1,106\n",
      "the           1,103\n",
      "hospital      2,704\n",
      "##ization     2,734\n",
      ",               117\n",
      "she           1,131\n",
      "had           1,125\n",
      "a               170\n",
      "L               149\n",
      "par          14,247\n",
      "##ot          3,329\n",
      "##ide         3,269\n",
      "##ct          5,822\n",
      "##omy        18,574\n",
      "for           1,111\n",
      "what          1,184\n",
      "turned        1,454\n",
      "out           1,149\n",
      "to            1,106\n",
      "be            1,129\n",
      "par          14,247\n",
      "##ot          3,329\n",
      "##id          2,386\n",
      "##itis       10,721\n",
      "and           1,105\n",
      "si           27,466\n",
      "##ala         5,971\n",
      "##den         2,883\n",
      "##itis       10,721\n",
      "with          1,114\n",
      "a               170\n",
      "large         1,415\n",
      "retained      5,366\n",
      "duct         26,862\n",
      "stone         2,576\n",
      ".               119\n",
      "Ultimately   16,266\n",
      ",               117\n",
      "it            1,122\n",
      "became        1,245\n",
      "clear         2,330\n",
      "she           1,131\n",
      "had           1,125\n",
      "no            1,185\n",
      "persistent   15,970\n",
      "infectious   20,342\n",
      "process       1,965\n",
      "in            1,107\n",
      "the           1,103\n",
      "par          14,247\n",
      "##ot          3,329\n",
      "##id          2,386\n",
      "bed           1,908\n",
      ",               117\n",
      "but           1,133\n",
      "had           1,125\n",
      "evolving     23,657\n",
      "car           1,610\n",
      "##ba          2,822\n",
      "##pen        11,741\n",
      "##em          5,521\n",
      "and           1,105\n",
      "c               172\n",
      "##ep          8,043\n",
      "##hal         7,654\n",
      "##os          2,155\n",
      "##por        18,876\n",
      "##in          1,394\n",
      "er           14,044\n",
      "##yt         25,669\n",
      "##hr          8,167\n",
      "##ode        13,040\n",
      "##rm          9,019\n",
      ".               119\n",
      "Her           1,430\n",
      "r               187\n",
      "##ash        10,733\n",
      "##es          1,279\n",
      "improved      4,725\n",
      "dramatically 12,235\n",
      "with          1,114\n",
      "transition    6,468\n",
      "to            1,106\n",
      "from          1,121\n",
      "me            1,143\n",
      "##rop        12,736\n",
      "##ene         7,582\n",
      "##m           1,306\n",
      "to            1,106\n",
      "c               172\n",
      "##ef         11,470\n",
      "##ep          8,043\n",
      "##ime        10,453\n",
      "to            1,106\n",
      "a               170\n",
      "##z           1,584\n",
      "##tre         7,877\n",
      "##ona         7,637\n",
      "##m           1,306\n",
      ".               119\n",
      "Her           1,430\n",
      "course        1,736\n",
      "was           1,108\n",
      "further       1,748\n",
      "complicated   8,277\n",
      "by            1,118\n",
      "a               170\n",
      "fever        10,880\n",
      "curve         7,660\n",
      "that          1,115\n",
      "had           1,125\n",
      "regular       2,366\n",
      "T               157\n",
      "##max        22,871\n",
      "in            1,107\n",
      "the           1,103\n",
      "101           7,393\n",
      "range         2,079\n",
      ",               117\n",
      "re            1,231\n",
      "##sol        24,313\n",
      "##ving        3,970\n",
      "while         1,229\n",
      "on            1,113\n",
      "van           3,498\n",
      "##com         8,178\n",
      "##y           1,183\n",
      "##cin        16,430\n",
      ",               117\n",
      "a               170\n",
      "##z           1,584\n",
      "##tre         7,877\n",
      "##ona         7,637\n",
      "##m           1,306\n",
      ",               117\n",
      "c               172\n",
      "##lind       27,969\n",
      "##am          2,312\n",
      "##y           1,183\n",
      "##cin        16,430\n",
      "and           1,105\n",
      "mi            1,940\n",
      "##ca          2,599\n",
      "##fu         14,703\n",
      "##ng          2,118\n",
      "##in          1,394\n",
      ",               117\n",
      "but           1,133\n",
      "[101, 4602, 1106, 1103, 2704, 2734, 117, 1131, 1125, 170, 149, 14247, 3329, 3269, 5822, 18574, 1111, 1184, 1454, 1149, 1106, 1129, 14247, 3329, 2386, 10721, 1105, 27466, 5971, 2883, 10721, 1114, 170, 1415, 5366, 26862, 2576, 119, 16266, 117, 1122, 1245, 2330, 1131, 1125, 1185, 15970, 20342, 1965, 1107, 1103, 14247, 3329, 2386, 1908, 117, 1133, 1125, 23657, 1610, 2822, 11741, 5521, 1105, 172, 8043, 7654, 2155, 18876, 1394, 14044, 25669, 8167, 13040, 9019, 119, 1430, 187, 10733, 1279, 4725, 12235, 1114, 6468, 1106, 1121, 1143, 12736, 7582, 1306, 1106, 172, 11470, 8043, 10453, 1106, 170, 1584, 7877, 7637, 1306, 119, 1430, 1736, 1108, 1748, 8277, 1118, 170, 10880, 7660, 1115, 1125, 2366, 157, 22871, 1107, 1103, 7393, 2079, 117, 1231, 24313, 3970, 1229, 1113, 3498, 8178, 1183, 16430, 117, 170, 1584, 7877, 7637, 1306, 117, 172, 27969, 2312, 1183, 16430, 1105, 1940, 2599, 14703, 2118, 1394, 117, 1133]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8689f804229e431daa6d9ecad3d65a73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3829875dd3274c668360b43719caa80b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-110-2e58918cac62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0msegments_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msegments_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bert-base-uncased'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/nlp_env/lib/python3.7/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    830\u001b[0m                     \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m                     \u001b[0mresume_download\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_download\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m                     \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m                 )\n\u001b[1;32m    834\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mresolved_archive_file\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/nlp_env/lib/python3.7/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, local_files_only)\u001b[0m\n\u001b[1;32m    688\u001b[0m             \u001b[0mresume_download\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_download\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m             \u001b[0muser_agent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muser_agent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 690\u001b[0;31m             \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    691\u001b[0m         )\n\u001b[1;32m    692\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_or_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/nlp_env/lib/python3.7/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mget_from_cache\u001b[0;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, local_files_only)\u001b[0m\n\u001b[1;32m    867\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s not found in cache or force_download set to True, downloading to %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 869\u001b[0;31m             \u001b[0mhttp_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresume_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_agent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muser_agent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    870\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storing %s in cache at %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/nlp_env/lib/python3.7/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, user_agent)\u001b[0m\n\u001b[1;32m    760\u001b[0m         \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_verbosity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNOTSET\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m     )\n\u001b[0;32m--> 762\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# filter out keep-alive new chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m             \u001b[0mprogress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/nlp_env/lib/python3.7/site-packages/requests/models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'stream'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    752\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mProtocolError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/nlp_env/lib/python3.7/site-packages/urllib3/response.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    574\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_fp_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 576\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecode_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/nlp_env/lib/python3.7/site-packages/urllib3/response.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m                 \u001b[0mcache_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfp_closed\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m                 if (\n\u001b[1;32m    521\u001b[0m                     \u001b[0mamt\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/nlp_env/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    455\u001b[0m             \u001b[0;31m# Amount is given, implement using readinto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/nlp_env/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    499\u001b[0m         \u001b[0;31m# connection, and the user is reading more bytes than will be provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m         \u001b[0;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 501\u001b[0;31m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m             \u001b[0;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/nlp_env/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/nlp_env/lib/python3.7/site-packages/urllib3/contrib/pyopenssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOpenSSL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSSL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSysCallError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Unexpected EOF\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/nlp_env/lib/python3.7/site-packages/OpenSSL/SSL.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1837\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSSL_peek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ssl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1838\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1839\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSSL_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ssl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1840\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_ssl_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ssl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "text = \"\"\"Prior to the hospitalization, she had a L parotidectomy for what turned out to be parotiditis \n",
    "    and sialadenitis with a large  retained duct stone. Ultimately, it became clear she had no persistent\n",
    "    infectious process in the parotid bed, but had evolving carbapenem and cephalosporin erythroderm.\n",
    "    Her rashes improved dramatically with transition to from meropenem to cefepime to aztreonam. Her course\n",
    "    was further complicated by a fever curve that had regular Tmax in the 101 range, resolving while on \n",
    "    vancomycin, aztreonam, clindamycin and micafungin, but  then recurred first low grade then becoming \n",
    "    very hectic and high  grade to 104 without any focal findings. The vancomycin was stopped and she \n",
    "    defervesced after 72 hours. She soon thereafter recovered her counts and all antibiotics were \n",
    "    discontinued when her ANC approached 500.\"\"\"\n",
    "bert_base_tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "# 1. Add the special tokens\n",
    "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "\n",
    "# 2. Split the sentence into tokens\n",
    "tokenized_text = bert_base_tokenizer.tokenize(marked_text)\n",
    "tokenized_text = tokenized_text[:150]\n",
    "\n",
    "# 3. Map the token strings to their vocabulary indices\n",
    "indexed_tokens = bert_base_tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "# 4. Display the words with their indices.\n",
    "for tup in zip(tokenized_text, indexed_tokens):\n",
    "    print('{:<12} {:>6,}'.format(tup[0], tup[1]))\n",
    "\n",
    "# 5. Segment IDs: 0 for sentence 1 and 1 for sentence 2\n",
    "segments_ids = [1] * len(tokenized_text)\n",
    "\n",
    "print(indexed_tokens)\n",
    "print(segments_ids)\n",
    "# 6. convert the lists to tensors\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    last_hidden_state, pooler_output = model(tokens_tensor, segments_tensors)\n",
    "last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 150, 768])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-trained BlueBERT model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "blue_bert_tokenizer = BertTokenizer.from_pretrained(\"/Users/kevin/Documents/data/BlueBERT/NCBI_BERT_pubmed_mimic_uncased_L-12_H-768_A-12/\")\n",
    "configuration = BertConfig.from_json_file(\"/Users/kevin/Documents/data/BlueBERT/NCBI_BERT_pubmed_mimic_uncased_L-12_H-768_A-12/bert_config.json\")\n",
    "model = BertModel.from_pretrained(\"/Users/kevin/Documents/data/BlueBERT/NCBI_BERT_pubmed_mimic_uncased_L-12_H-768_A-12/pytorch_model.bin\", config=configuration)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer requires input text to be either a single string or list of strings\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "tokens = blue_bert_tokenizer(\n",
    "    text_data[:10].values.tolist(),\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    max_length=128,\n",
    "    return_tensors=\"pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 128])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing batch 1...\n",
      "Tokenizing batch 101...\n",
      "Tokenizing batch 201...\n",
      "Tokenizing batch 301...\n",
      "Tokenizing batch 401...\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "max_seq_length = 64\n",
    "token_li = []\n",
    "for i in range(text_data.shape[0] // batch_size + 1):\n",
    "    start_idx = i * 32\n",
    "    stop_idx = (i + 1) * 32\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(f\"Tokenizing batch {i + 1}...\")\n",
    "    # shape: (32, 128)\n",
    "    tokens = blue_bert_tokenizer(\n",
    "        text_data[start_idx:stop_idx].values.tolist(),\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_seq_length,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    token_li.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1372"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1372*32\n",
    "len(token_li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding batch 1...\n",
      "Embedding batch 26...\n",
      "Embedding batch 51...\n",
      "Embedding batch 76...\n",
      "Embedding batch 101...\n",
      "Embedding batch 126...\n",
      "Embedding batch 151...\n",
      "Embedding batch 176...\n",
      "Embedding batch 201...\n",
      "Embedding batch 226...\n",
      "Embedding batch 251...\n",
      "Embedding batch 276...\n",
      "Embedding batch 301...\n",
      "Embedding batch 326...\n",
      "Embedding batch 351...\n",
      "Embedding batch 376...\n",
      "Embedding batch 401...\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "hidden_states = []\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(token_li):\n",
    "        if i % 25 == 0:\n",
    "            print(f\"Embedding batch {i + 1}...\")\n",
    "        last_hidden_state, _ = model(**batch)\n",
    "        hidden_states.append(last_hidden_state)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13162, 64, 768])\n"
     ]
    }
   ],
   "source": [
    "embeddings = torch.cat(hidden_states)\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_list = [0,1,2,3,4,5,6,7,8]\n",
    "my_list[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28584"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(vocab.keys()) & set(wv.vocab.keys()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x83 in position 10: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-5f18262563f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# wv = gensim.models.KeyedVectors.load_word2vec_format('/Users/kevin/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz', binary=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# BlueBERT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mwv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Users/kevin/gensim-data/NCBI_BERT_pubmed_mimic_uncased_L-12_H-768_A-12.zip'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/nlp_env/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m   1496\u001b[0m         return _load_word2vec_format(\n\u001b[1;32m   1497\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1498\u001b[0;31m             limit=limit, datatype=datatype)\n\u001b[0m\u001b[1;32m   1499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_keras_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/nlp_env/lib/python3.7/site-packages/gensim/models/utils_any2vec.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m    341\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loading projection weights from %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m         \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m         \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# throws for invalid file format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/nlp_env/lib/python3.7/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36many2unicode\u001b[0;34m(text, encoding, errors)\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0municode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x83 in position 10: invalid start byte"
     ]
    }
   ],
   "source": [
    "wv = gensim.models.KeyedVectors.load_word2vec_format('/Users/kevin/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz', binary=True)\n",
    "# BlueBERT\n",
    "# wv = gensim.models.KeyedVectors.load_word2vec_format('/Users/kevin/gensim-data/NCBI_BERT_pubmed_mimic_uncased_L-12_H-768_A-12.zip', binary=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vitals\n",
      "lf\n",
      "bs\n",
      "cxr\n",
      "found\n",
      "parents\n",
      "guarding\n",
      "aspirin\n",
      "clear\n",
      "nad\n"
     ]
    }
   ],
   "source": [
    "for i, word in enumerate(vocab):\n",
    "    if i == 10:\n",
    "        break\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'car'\t'vehicle'\t0.78\n",
      "'car'\t'minivan'\t0.69\n",
      "'car'\t'bicycle'\t0.54\n",
      "'car'\t'airplane'\t0.42\n",
      "'car'\t'cereal'\t0.14\n",
      "'car'\t'communism'\t0.06\n"
     ]
    }
   ],
   "source": [
    "pairs = [\n",
    "    ('car', 'vehicle'),\n",
    "    ('car', 'minivan'),   # a minivan is a kind of car\n",
    "    ('car', 'bicycle'),   # still a wheeled vehicle\n",
    "    ('car', 'airplane'),  # ok, no wheels, but still a vehicle\n",
    "    ('car', 'cereal'),    # ... and so on\n",
    "    ('car', 'communism'),\n",
    "]\n",
    "for w1, w2 in pairs:\n",
    "    print('%r\\t%r\\t%.2f' % (w1, w2, wv.similarity(w1, w2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word2vec(tokens_list, vector, generate_missing=False, k=300):\n",
    "    if generate_missing:\n",
    "        vectorized = [vector[word] if word in vector else np.random.uniform(-0.25,0.25,k) for word in tokens_list]\n",
    "    else:\n",
    "        vectorized = [vector[word] if word in vector else np.zeros(k) for word in tokens_list]\n",
    "    return vectorized\n",
    "sample_sequence = get_word2vec(tokens[0], wv, generate_missing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(sample_sequence)\n",
    "sample_sequence[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(nn.Module):\n",
    "    \"\"\" Text CNN from Yoon Kim's 2014 paper: https://arxiv.org/pdf/1408.5882.pdf\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    sequence_length : int\n",
    "        The length (in words/tokens) of each sentence.\n",
    "    num_classes : int\n",
    "        Number of output classes.\n",
    "    vocab_size : int\n",
    "        The number of unique tokens in our vocabulary.\n",
    "    embedding_size: int\n",
    "        The vector length for word embeddings. (standard word2vec is 300, BERT is 768)\n",
    "    num_filters: int\n",
    "        The number of filters to apply.\n",
    "    kernel_sizes: tuple(int)\n",
    "        The kernel size for each desired filter (e.g. [3,4,5])\n",
    "    dropout_rate: float\n",
    "        Probability of dropping a neuron in the dropout layer.  Must be in the range [0.0, 1.0]\n",
    "        Default = 0.5\n",
    "    embedding_weights: torch.FloatTensor\n",
    "        Pre-trained embedding weights to optionally pass, otherwise embedding weights will be learned.\n",
    "        Default is None.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    model : nn.Module\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes,\n",
    "        embedding_size,\n",
    "        num_filters,\n",
    "        kernel_sizes=(3,4,5),\n",
    "        dropout_rate=0.5,\n",
    "    ):\n",
    "        super(TextCNN, self).__init__()\n",
    "        \n",
    "        # convolutional layer\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(\n",
    "            in_channels=1,\n",
    "            out_channels=num_filters,\n",
    "            kernel_size=(kernel_size, embedding_size),\n",
    "        ) for kernel_size in kernel_sizes])\n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # fully connected layer\n",
    "        self.fc = nn.Linear(\n",
    "            in_features=num_filters * len(kernel_sizes),\n",
    "            out_features=num_classes\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # (batch_size, in_channels, sequence_length, embedding_size)\n",
    "        \n",
    "        x_li = []\n",
    "        for conv in self.convs:\n",
    "            _x = F.relu(conv(x)) # (batch_size, out_channels, sequence_length, 1)\n",
    "            _x = _x.squeeze(3) # (batch_size, out_channels, sequence_length)\n",
    "            _x = F.max_pool1d(_x, _x.size(2)).squeeze(2) # (batch_size, out_channels)\n",
    "            x_li.append(_x)\n",
    "            \n",
    "        x = torch.cat(x_li, 1)\n",
    "        x = self.dropout(x) # (batch_size, len(kernel_sizes) * out_channels)\n",
    "        logits = self.fc(x) # (batch_size, num_classes)\n",
    "        \n",
    "        probs = F.softmax(logits, dim=1) # (batch_size, num_classes)\n",
    "        classes = torch.max(probs, 1)[1] # (batch_size)\n",
    "        \n",
    "        return probs, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert training data to pytorch dataset\n",
    "train_tensors = torch.utils.data.TensorDataset(\n",
    "    embeddings,\n",
    "    torch.tensor(y.values).long()\n",
    ")\n",
    "\n",
    "# Create iterable\n",
    "trainloader = torch.utils.data.DataLoader(train_tensors, batch_size=32,\n",
    "                                          shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model = TextCNN(\n",
    "        num_classes=2,\n",
    "        embedding_size=768,\n",
    "        num_filters=128,\n",
    "        dropout_rate=0.5,\n",
    "    )\n",
    "    \n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "            \n",
    "    for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "        running_loss = 0\n",
    "        running_corrects = 0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward\n",
    "            probs, classes = model(inputs)\n",
    "            # backprop\n",
    "            loss = loss_function(probs, labels)\n",
    "            loss.backward()\n",
    "            # update/optimize\n",
    "            optimizer.step()\n",
    "\n",
    "#             running_losses.append(loss.data[0])\n",
    "#             if i % 50 == 0:\n",
    "#                 disp_loss = sum(running_losses) / len(running_losses)\n",
    "# #                 writer.add_scalar(\"train/loss\", loss, step)\n",
    "#                 logging.info(\"step = {}, loss = {}\".format(i, loss))\n",
    "#                 running_losses = []\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            running_corrects += torch.sum(classes == labels.data)\n",
    "            if i % 50 == 0 and i != 0:    # print every 50 mini-batches\n",
    "                print('[%d, %5d] loss: %.3f acc %.3f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 50, running_corrects / 50))\n",
    "                running_loss = 0.0\n",
    "                running_corrects = 0.0\n",
    "                \n",
    "#             epoch_loss = running_loss / dataset_sizes[phase]\n",
    "#             epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevin/opt/miniconda3/envs/nlp_env/lib/python3.7/site-packages/ipykernel_launcher.py:74: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    51] loss: 0.383 acc 30.000\n",
      "[1,   101] loss: 0.378 acc 29.940\n",
      "[1,   151] loss: 0.390 acc 29.540\n",
      "[1,   201] loss: 0.381 acc 29.840\n",
      "[1,   251] loss: 0.377 acc 29.960\n",
      "[1,   301] loss: 0.383 acc 29.780\n",
      "[1,   351] loss: 0.376 acc 29.980\n",
      "[1,   401] loss: 0.369 acc 30.220\n",
      "[2,    51] loss: 0.384 acc 30.000\n",
      "[2,   101] loss: 0.392 acc 29.480\n",
      "[2,   151] loss: 0.368 acc 30.240\n",
      "[2,   201] loss: 0.385 acc 29.720\n",
      "[2,   251] loss: 0.380 acc 29.860\n",
      "[2,   301] loss: 0.381 acc 29.840\n",
      "[2,   351] loss: 0.373 acc 30.080\n",
      "[2,   401] loss: 0.368 acc 30.240\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
