{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from string import punctuation\n",
    "\n",
    "import gensim\n",
    "from nltk import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Admissions Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strict copd coding\n",
    "strict_icd9 = [\n",
    "    \"49120\",\n",
    "    \"49121\",\n",
    "    \"49122\",\n",
    "    \"49320\",\n",
    "    \"49321\",\n",
    "    \"49322\",\n",
    "    \"496\",\n",
    "]\n",
    "\n",
    "# regular copd coding\n",
    "reg_icd9 = [\n",
    "    \"4911\",\n",
    "    \"4920\",\n",
    "    \"4928\",\n",
    "]\n",
    "\n",
    "df = pd.read_csv(\"~/Documents/data/mimic/DIAGNOSES_ICD.csv\")\n",
    "n = df.HADM_ID.nunique()\n",
    "copd_hadmids = df[df.ICD9_CODE.isin(strict_icd9 + reg_icd9)].HADM_ID.unique()\n",
    "n_copd = len(copd_hadmids)\n",
    "print(f\"Admission: {n}\")\n",
    "print(f\"COPD Admissions: {n_copd}\")\n",
    "\n",
    "patients = pd.read_csv(\"~/Documents/data/mimic/PATIENTS.csv\", parse_dates=['DOB', 'DOD', 'DOD_HOSP'])\n",
    "print(f\"Num patients: {patients['SUBJECT_ID'].nunique()}\")\n",
    "print(f\"Num female: {patients[patients['GENDER'] == 'F']['SUBJECT_ID'].nunique()}\")\n",
    "\n",
    "admission_cols = [\n",
    "    'HADM_ID',\n",
    "    'ADMISSION_TYPE',\n",
    "    'ADMITTIME',\n",
    "    'DISCHTIME',\n",
    "    'DEATHTIME',\n",
    "    'EDREGTIME',\n",
    "    'EDOUTTIME',\n",
    "    'HOSPITAL_EXPIRE_FLAG',\n",
    "    'HAS_CHARTEVENTS_DATA',\n",
    "]\n",
    "\n",
    "tmp = pd.read_csv(\"~/Documents/data/mimic/ADMISSIONS.csv\", parse_dates=['ADMITTIME', 'DISCHTIME','DEATHTIME', 'EDREGTIME', 'EDOUTTIME',])[admission_cols]\n",
    "\n",
    "# concat primary dx onto admissions\n",
    "admits = tmp.merge(df, on=['HADM_ID']).drop_duplicates(subset=[\"HADM_ID\"])\n",
    "\n",
    "# get rid of spurrious admissions and ignore newborns\n",
    "admits = admits[(admits['DISCHTIME'] > admits['ADMITTIME']) & (admits.ADMISSION_TYPE != \"NEWBORN\")]\n",
    "\n",
    "# add age information\n",
    "admits = admits.merge(patients[['SUBJECT_ID', 'DOB']], on='SUBJECT_ID', how='left')\n",
    "admits['age'] = admits.apply(lambda x: (x['ADMITTIME'].date() - x['DOB'].date()).days // 365.242, axis=1)\n",
    "\n",
    "# tag copd admissions\n",
    "admits['copd'] = admits.HADM_ID.isin(copd_hadmids)\n",
    "\n",
    "# get the type and time of the next admission\n",
    "admits.sort_values(by=['SUBJECT_ID', 'ADMITTIME'],inplace=True)\n",
    "admits['next_admit_time'] = admits.groupby('SUBJECT_ID').ADMITTIME.shift(-1)\n",
    "admits['next_admit_type'] = admits.groupby('SUBJECT_ID').ADMISSION_TYPE.shift(-1)\n",
    "# if the next admission is elective, nullify and back fill\n",
    "admits.loc[admits.next_admit_type == \"ELECTIVE\", 'next_admit_time'] = pd.NaT\n",
    "admits.loc[admits.next_admit_type == \"ELECTIVE\", 'next_admit_type'] = np.nan\n",
    "admits[['next_admit_time','next_admit_type']] = admits.groupby(['SUBJECT_ID'])[['next_admit_time','next_admit_type']].fillna(method = 'bfill')\n",
    "\n",
    "# compute readmission stats\n",
    "admits['readmit_time'] = admits.groupby('SUBJECT_ID').apply(lambda x: x['next_admit_time'] - x['DISCHTIME']).reset_index(level=0, drop=True)\n",
    "admits['7d_readmit'] = (admits['readmit_time'].dt.total_seconds() < 7 * 24 * 3600).astype(int)\n",
    "admits['30d_readmit'] = (admits['readmit_time'].dt.total_seconds() < 30 * 24 * 3600).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_summary(df):\n",
    "    gb = df.groupby(['copd','7d_readmit']).HADM_ID.count()\n",
    "    non_rate = gb[0][1] / gb[0].sum()\n",
    "    copd_rate = gb[1][1] / gb[1].sum()\n",
    "    print(\"Non-COPD 7d readmit rate: {:.1%}\".format(non_rate))\n",
    "    print(\"COPD 7d readmit rate:     {:.1%}\".format(copd_rate))\n",
    "    print('')\n",
    "\n",
    "    gb = df.groupby(['copd','30d_readmit']).HADM_ID.count()\n",
    "    non_rate = gb[0][1] / gb[0].sum()\n",
    "    copd_rate = gb[1][1] / gb[1].sum()\n",
    "    print(\"Non-COPD 30d readmit rate: {:.1%}\".format(non_rate))\n",
    "    print(\"COPD 30d readmit rate:     {:.1%}\".format(copd_rate))\n",
    "    print('')\n",
    "\n",
    "    gb = df[df.DEATHTIME.notnull()].drop_duplicates(subset=['SUBJECT_ID']).groupby('copd').size()\n",
    "    print(\"Non-COPD mortality rate: {:.1%}\".format(gb[0] / df[df.copd == False].shape[0]))\n",
    "    print(\"COPD mortality rate:     {:.1%}\".format(gb[1] / df[df.copd].shape[0]))\n",
    "\n",
    "print(\"<65 Admissions\")\n",
    "print(\"-\"*25)\n",
    "print_summary(admits[admits['age'] < 65])\n",
    "\n",
    "print(\"\\n\\n65+ Admissions\")\n",
    "print(\"-\"*25)\n",
    "print_summary(admits[admits['age'] >= 65])\n",
    "\n",
    "print(\"\\n\\nAll Admissions\")\n",
    "print(\"-\"*25)\n",
    "print_summary(admits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discharge Notes Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subjects that died in the hosp\n",
    "deceased_subj_ids = admits[admits.DEATHTIME.notnull()].SUBJECT_ID.unique()\n",
    "# subjects w/ at least one copd related admission\n",
    "copd_subj_ids = admits[admits.copd].SUBJECT_ID.unique()\n",
    "# all admissions for subjects w/ at least one copd related admission\n",
    "hadm_ids_w_copd = admits[admits.SUBJECT_ID.isin(copd_subj_ids)].HADM_ID.unique()\n",
    "\n",
    "\n",
    "print('Loading medical notes...')\n",
    "\n",
    "chunk_reader = pd.read_csv(\"~/Documents/data/mimic/NOTEEVENTS.csv\", chunksize=100000, usecols=['SUBJECT_ID','HADM_ID', 'CHARTDATE','CATEGORY', 'DESCRIPTION', 'TEXT',])\n",
    "chunk_li = []\n",
    "iteration = 0\n",
    "for chunk in chunk_reader:\n",
    "    \n",
    "    if iteration % 5 == 0:\n",
    "        print(f\"Iteration {iteration}\")\n",
    "        # keep only admissions for subjects that had at least one copd admit\n",
    "#     chunk_li.append(chunk[(chunk['HADM_ID'].isin(hadm_ids_w_copd))])\n",
    "        # keep just the discharge summaries\n",
    "        chunk_li.append(chunk[(chunk['CATEGORY'] == 'Discharge summary')])\n",
    "    iteration += 1\n",
    "    \n",
    "print(\"Done.\")\n",
    "\n",
    "notes = pd.concat(chunk_li, ignore_index=True)\n",
    "# keep only one discharge summary per admission\n",
    "notes = notes.sort_values(by=['SUBJECT_ID', 'HADM_ID', 'CHARTDATE']).groupby(['HADM_ID']).nth(-1)\n",
    "cols = ['HADM_ID', 'SUBJECT_ID','age', 'copd', 'HOSPITAL_EXPIRE_FLAG','ADMISSION_TYPE', 'ADMITTIME', 'DISCHTIME', 'DEATHTIME','next_admit_time', 'next_admit_type','30d_readmit',]\n",
    "notes = notes.merge(admits[cols], on=['SUBJECT_ID','HADM_ID'], how='inner')\n",
    "notes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Process Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(df):\n",
    "    # This function preprocesses the text by filling not a number \n",
    "    # and replacing new lines ('\\n') and carriage returns ('\\r')\n",
    "    df.TEXT = df.TEXT.fillna(' ')\n",
    "    df.TEXT = df.TEXT.str.replace('\\n',' ')\n",
    "    df.TEXT = df.TEXT.str.replace('\\r',' ')\n",
    "    return df\n",
    "\n",
    "X = preprocess_text(notes[notes.HOSPITAL_EXPIRE_FLAG == 0]).TEXT\n",
    "y = notes[notes.HOSPITAL_EXPIRE_FLAG == 0]['30d_readmit']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stop_words = ['the','and','to','of','was','with','a','on','in','for','name','is','patient','s','he','at',\n",
    "                  'as','or','one','she','his','her','am','were','you','pt','pm','by','be','had','your','this',\n",
    "                  'date','from','there','an','that','p','are','have','has','h','but','o','namepattern','which',\n",
    "                  'every','also','should','if','it','been','who','during', 'x']\n",
    "\n",
    "def basic_tokenizer(text, stop_words=[]):\n",
    "    # tokenize the text by replacing punctuation and numbers with spaces and lowercase all words\n",
    "    punc_list = punctuation+'0123456789'\n",
    "    t = str.maketrans(dict.fromkeys(punc_list, \" \"))\n",
    "    text = text.lower().translate(t)\n",
    "    tokens = word_tokenize(text)\n",
    "    return [t for t in tokens if t not in stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = gensim.models.KeyedVectors.load_word2vec_format('/Users/kevin/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
